Notes on Azure
Virtual Machine types:
Reference: https://azure.microsoft.com/en-in/pricing/details/virtual-machines/series/
Create a vm depending on your workload requirement and choose configure access to the vm by using ASG and NSG.

AZURE VNET:
ASG: This is used to group diff application or vm to a consolidated set of security group.
It enhances NSG, you'll have to use ASG in collaboration with NSG. ASG will select all vm and you can use NSG to set traffic flow from ASG security group.


**Azure Virtual Network (VNet):**
A VNet is an isolated network within the Azure cloud that allows various Azure resources, such as VMs (Virtual Machines), to securely communicate with each other,
the internet, and on-premises networks. It's the fundamental building block for your private network in Azure.

**Network Security Groups (NSG):**
NSGs are used to filter network traffic to and from Azure resources in an Azure VNet. An NSG can contain multiple inbound and outbound security rules that allow
or deny traffic based on several parameters, such as protocol, port, and source or destination IP address. NSGs can be associated with subnets
in the VNet or directly to individual resources.

**Application Security Groups (ASG):**
ASGs are a feature that enhances the capabilities of NSGs, allowing you to define fine-grained network security policies based on workloads or applications.
 Instead of defining rules based on IP addresses, you can group virtual machines and define network security policies based on these groups.
 This abstraction simplifies the management and maintenance of network security rules, as you don't need to update rules when adding or removing VMs in your application group.

**How ASGs Enhance NSGs:**
ASGs are used in conjunction with NSGs to refine how security rules are applied. When you associate a VM to an ASG,
you can then use that ASG as a source or destination in NSG security rules. This setup allows for more granular control over
traffic flow between different groups of VMs or applications within your Azure VNet. For instance, you can easily allow communication
between your front-end and back-end layers without having to specify individual IP addresses.

By utilizing ASGs with NSGs, you can:
- Dynamically manage security policies as you scale your environment up or down.
- Apply and maintain security rules more efficiently by grouping VMs according to their roles, applications, or workloads.
- Improve the clarity of your security rules, making it easier to understand and audit the traffic flow within your VNet.

In summary, ASGs help organize your virtual machines into logical groups that can be referenced in NSG rules, simplifying the management
and application of network security policies in a dynamic cloud environment.



Important:
  A firewall and a Web Application Firewall (WAF) serve different purposes, and in the context of Azure, they are distinct offerings designed
  for different security needs.

### Azure Firewall

Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It's a stateful firewall
as a service with built-in high availability and

unrestricted cloud scalability. Azure Firewall provides outbound, inbound, and network-level traffic filtering and monitoring.
 It can filter traffic between resources in a VNet, from the internet to the VNet, and even between different VNets. Key features include:
- **FQDN filtering in network rules**
- **Network traffic filtering rules**
- **SNAT support for outbound internet connectivity**
- **Integrated threat intelligence based on Microsoft Threat Intelligence**
- **High availability built-in**
- **Unrestricted cloud scalability**

### Azure Web Application Firewall (WAF)

Azure Web Application Firewall (WAF) is a specialized form of firewall focused on securing web applications. It is offered as part of Azure Application Gateway,
 Azure Front Door Service, and Azure Content Delivery Network (CDN) services. WAF is designed to protect web applications from common exploits and vulnerabilities,
 such as SQL injection, cross-site scripting (XSS), and other web-based attacks. WAF operates at Layer 7 (HTTP/HTTPS)
 and uses rules from the OWASP (Open Web Application Security Project) core rule sets to identify and block malicious traffic. Key features include:
- **Protection against web vulnerabilities and attacks**
- **Custom rules and managed rule sets**
- **Monitoring and logging capabilities**
- **Integration with Azure Application Gateway, Front Door, and CDN**

### Key Differences

- **Scope of Protection**: Azure Firewall provides broad network-level protection and monitoring for resources within
Azure Virtual Networks, including filtering outbound, inbound, and internal network traffic. On the other hand, Azure WAF
 is specifically designed to protect web applications from common web vulnerabilities and attacks.
- **Layer of Operation**: Azure Firewall operates at the network layer (Layers 3 and 4), while Azure WAF operates at the
application layer (Layer 7) to inspect HTTP/HTTPS traffic.
- **Use Cases**: Use Azure Firewall when you need general network security and traffic filtering for all types of resources in your VNet.
 Choose Azure WAF when you specifically need to protect web applications from attacks and vulnerabilities.

In summary, while both services provide security features, they cater to different aspects of network and application security within Azure.
It's not uncommon for organizations to use both Azure Firewall and Azure WAF together to achieve comprehensive security coverage for their cloud resources and applications.

Configure DNAT rule on azure:
to allow traffic from source * or my ip,  destination ip address <firewall public ip> protocol <tcp> destination port <4000>  translated type <ip address> translated address <private ip of the vm>

AZURE STORAGE:
NB: Blob storage --> This also means containers

 Automate Azure Resources using Azure CLI:
   Install or upgrade azure cli.
   check current version run; az version
Reference:
  https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-macos#update
  Using the CLI to create resources is prone to error, we can introduce a bit of automation by using the CLI to create diff resources.

Common Azure CLI commands for managing Azure resources:
Reference:
  https://learn.microsoft.com/en-us/azure/virtual-machines/linux/cli-manage
  This ensures automation, reduction of error and efficiency.

  ERROR WHY LOGIN IN TO AZURE:

➜  Azure-series git:(main) ✗ ./create_vm.sh
AADSTS50076: Due to a configuration change made by your administrator, or because you moved to a new location, you must use multi-factor authentication to access '797f4846-ba00-4fd7-ba43-dac1f8f63013'. Trace ID: 3a8be455-0c91-4eb0-95fb-f1d78e1e2f00 Correlation ID: cfc550eb-e6b2-4928-8a22-6dd7135e377e Timestamp: 2024-03-21 16:32:19Z
Interactive authentication is needed. Please run:
az login --scope https://management.core.windows.net//.default

resolved by running:
az login --tenant <tenant-id>
This is b/c mfa is needed, a code will be sent to the email act associated with your azure act.
or run

Interactive authentication is needed. Please run:
az login --scope https://management.core.windows.net//.default
This will resolve the issue..


Manage azure subscription:
To switch between Azure subscriptions using the Azure CLI, you can use the `az account set` command followed by the subscription
ID or name of the subscription you want to switch to. Here's the general syntax:

```
az account set --subscription SUBSCRIPTION_ID_OR_NAME
```

Replace `SUBSCRIPTION_ID_OR_NAME` with either the subscription ID or the name of the subscription you want to switch to.
You can find the subscription ID and names of your subscriptions by running the following command:

```
az account list --output table
```

This command will list all the subscriptions associated with your account along with their IDs and names.
Once you have the subscription ID or name, you can use the `az account set` command to switch to that subscription. Here's an example:

```
az account set --subscription "My Subscription"
```

Replace `"My Subscription"` with the name of the subscription you want to switch to.

After running the `az account set` command, your default subscription will be updated, and subsequent Azure CLI commands will be executed in the context of the newly selected subscription.
---
az account show --output table
This command will display information about the currently selected subscription, including its ID,
name, and other details, in a tabular format. The subscription marked as (current) is the one currently in use.

Create resource Group:
az group create --name myResourceGroup --location eastus
Reference:
  https://learn.microsoft.com/en-us/cli/azure/manage-azure-groups-azure-cli

  To ssh into the machine created from your script.
  click connect on the console and copy the command ex:
  click on Native SSH
  ssh -i ~/.ssh/id_rsa.pem wiz_obi@52.255.174.251
  The user and the ip will be there

ARM template:
az group create --name arm-vscode --location eastus
az deployment group create --resource-group arm-vscode --template-file azuredeploy.json --parameters azuredeploy.parameters.json

Useful links:
  https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/template-tutorial-add-parameters?tabs=azure-cli
  https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/quickstart-create-templates-use-visual-studio-code?tabs=CLI
  https://learn.microsoft.com/en-us/azure/templates/microsoft.storage/storageaccounts?pivots=deployment-language-arm-template

  We have the.
  1. parameters --> repititive names, that can be used accross env
  2. funtions -->
  3. resources  --> service or resource to create
  4. variables --> container to store dynamic values
  5. outputs --> Things you want to get out of the created resource, say public ip of an vm

Azure IAM from Basics
Azure Managed Identities Demo with Microsoft Entra
Authentications: You have users and groups and authorizations we have actions that can be performed by the authenticated entities..
Authorizations we have roles..
When you create a user account with the root user and sign into that new account, you have sucessfully authenticated into the azure account but you
dont have any role yet.. As a result you can perform any action like creating resources under that account.
When you have been assigned a role to create resources then that is authorization.

Important: You can create custom role and privileges and assign it to a user or group. To create a custom role you have to activate microsoft entra id premium p1 or p2.
Group: This is used to assign roles to group of users.
say you have devops and developers group, instead of creating and assign individual roles to users, you can create a group, add roles to the group.
Then add the users to that group and theyll inherit the permission or role associated to that group..
1 Authentications
2 Authorizations
3 roles
4 users
5 groups

Important: How about resources trying to access each other?
Say you have a requirment that a vm needs access to a blob storage..
How do you implement this? We use service principle and managed Identities.
As stated earlier, role is what a user or service can do, action the user can perform on a resource or action a service can perform on another service.
In terms of service to service interactions we use service principle and managed Identities...
We create a service principle or managed Identities for the vm to access data in the blob storage.
This role is assigned to the entity/service that needs to access the service.
Since vm wants to access resources in blob storage service, we then create a role that will enable vm to do this and assign it to the vm..

Important: when a service wants to talk to other service, in azure we use the concept of.
1. service principle --> This is created and managed by the user and access of the service principle rotated by the user.
2. managed Identities --> This azure manages the access rotation once a user creates it.
Summarily ..
grant a role to the vm called managed identity to be able to access blob containers..
NB: We have system assigned managed identity and user assigned managed identity.
When you enable system assigned managed identity, the system automatically assigns the service or resource a role based on the managed identity.
also note that an Object pinciple ID is generated.
<<<<<<< HEAD
Go to the storage account and add role assignment..

A sample script is on day 12.

AZURE DEVOPS:
---------------------------------
Architecture
1. Overview
2. Boards --> A typical example is Jira
3. Repos --> Used for the storage of application code, a source code repository.
4. Pipelines --> CICD section, you define stages, jobs and steps
5. Test Plans --> A central location where your team can coordinate all your manual test activities, track progress, and get critical insights.
 As a user with basic access level, here is how you can get started right away.

6. Artifacts --> Used for the storage of cicd endproduct or built artifact..

Use Azure Pipelines to support the following scenarios:

    Works with any language or platform
    Deploys to different types of targets at the same time
    Integrates with Azure deployments
    Builds on Windows, Linux, or Mac machines
    Integrates with GitHub
    Works with open-source projects

Reference:
  https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops
  https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/yaml-pipeline-editor?view=azure-devops

  Lets deploy the example voting app on azure devops:
  First test the application locally and see that it works b4 putting it to a cicd pipelines.

  1. Create organization,
  2. create project
  3. import your project code from github to azure devops repo.
  4. create a azure container registry to store built images.
  5. create pipelines for the 3 microservice applications, worker, result and vote.
  First complete the ci jobs which is running of unit test, static test, build of image and pushing built image to the image registry.

  Reference:
    https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/containers/build-image?view=azure-devops

    sample yaml file for the cicd pipeline
    # Docker
# Build and push an image to Azure Container Registry
# https://docs.microsoft.com/azure/devops/pipelines/languages/docker

trigger:
- main

resources:
- repo: self

variables:
  # Container registry service connection established during pipeline creation
  dockerRegistryServiceConnection: '51f380a4-6599-4df5-b1cb-8d8880badb14'
  imageRepository: 'nikesales'
  containerRegistry: 'nikesalescicd.azurecr.io'
  dockerfilePath: '$(Build.SourcesDirectory)/result/Dockerfile'
  tag: '$(Build.BuildId)'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

stages:
- stage: Build
  displayName: Build and push stage
  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName) #azure do not provide runner or pool for a free account..
    steps:
    - task: Docker@2
      displayName: Build and push an image to container registry
      inputs:
        command: buildAndPush
        repository: $(imageRepository)
        dockerfile: $(dockerfilePath)
        containerRegistry: $(dockerRegistryServiceConnection)
        tags: |
          $(tag)

Overview,
trigger:
- main
You can also specify on which action "trigger" should the pipeline run?
Since we have 3 microservices, you define that if a change is made to a specific code of one of the microservice that the pipeline should be triggered,
say a change to the vote, worker or result app code, a pipeline run should be triggered.
This way you wont be building the entire microservice code.. If a change got made in the vote, pipelines run same for result and worker.
As you know in some cases changes cant be done in all microservices at once.. Specify the path in the app code that once change is made the pipeline gets triggered....

Demands and capabilities:
Reference:
  https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/pool-demands?view=azure-pipelines

  Important:
  If you created a pool to run you jobs, ensure you register the name of the agent in the pipeline after Referencing it in the pipeline yaml.
  go to settings, enter the vm or pool name and generate a code to run on you vm to enable it authenticate with the pipeline..

  Reference configuration for the pool:
    https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/linux-agent?view=azure-devops
    Important:
    Create a PAT that will be used to configure the pool
    use agent pool server url while authenticating with the vm as
    Server URL
Azure Pipelines: https://dev.azure.com/{your-organization}

Important:
ECR registry, on the azure Devops pipeline, click on settings and click on Service connections --> New connections.
Then click docker registry --> choose either azure or docker at this point ---> for azure choose Authentication Type which can be
1 service principle or managed service Identities.---> Service connection name <this can be any name>.
This is how you authenticate your azure or any registry to your azure pipeline.

Reference:
  https://stackoverflow.com/questions/59439705/what-is-a-docker-registry-service-connection

  CONTINUOS DELIVERY PIPELINE FOR VOTING APP..
  We used azure devops for the CI, we will use argocd for the cd.. It is a Gitops pull mechanism
  We will authenticate argocd with azure devops repo to monitor for changes in our source code and then deploy our app to kubernetes cluster.
  1. create Azure kubernetes cluster.
  2. While creating AKS, we require agentPool, which is vm.. This is b/c AKS is a managed service and to run our workloads, pods, containers, applications we need a vm..
  agentpool in aks is just like nodegroup in eks, theyre all used to group vm to run our k8s workloads..
  3. set the agentpool, min 1 and max of 2 depending on your requirment.. This is b/c we will use azure scaleset where the vm can scale up based on demand.
  Important:
  you can have diff agentpool,
  1. system agentpool
  2. user agentpool
  theyre can be used to manage diff aspect of your workload..

  NB: validating your aks creation, if you run into resource quota issue, change the region..

  Question:
  Why gitops?
  why shell scripts?
  There is another resource in gitops called argo image updater but its not in GA at the moment.
  Shell script is generic and the agent is linux vm reason for shell script..
  This is why we use python/shell script.
  Why gitops?
  Continous reconciliation, argocd watches the repo and make the desired state the actual state of the cluster.
  It has the sourcecode as it sole point of truth. changes made directly to the cluster is reverted.
  2. deploy argocd in the kubernetes cluster.
  3. configure argocd
  4. create update pipeline for the cd jobs, this will monitor the said path in the repo for changes and this then gets deployed into the k8s cluster.
  NB:
    Before proceeding, you have to install azure cli, login to the sub for the azureagent to manage the aks and deploy app see below
  -------


  Reference to argocd docs.
  https://argo-cd.readthedocs.io/en/stable/
  Note theres no direct connection between the ci and the cd with argocd, argocd handles the cd by monitoring changes in our source code repo and make our desired state the actual state
  of the kubernetes cluster..
  AKS command to run in azureagent:
  https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials
  commands:

  1. install azure cli
  https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt
  curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

  2. login to your azure sub
  3. az account set --subscription 223b3de2-144c-49ec-93dc-765b9cebf465
  4. az aks get-credentials --resource-group stans-robot-shop --name stans-robot-shop-demo --overwrite-existing

  5 install kubectl
 Run: sudo az aks install-cli --client-version=1.27.9
https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli


If your repository is hosted on Azure DevOps instead of GitHub, you would need to adjust the URL and potentially the authentication method in the Secret accordingly.
Azure DevOps uses different URLs and authentication mechanisms compared to GitHub. Here's how you can modify the Secret manifest:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: argocd-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository
stringData:
  type: git
  url: https://dev.azure.com/YourOrganization/YourProject/_git/YourRepo
  token: <your-personal-access-token>
```

Here's what changed:

- `url`: The URL of the Azure DevOps repository.
Replace `YourOrganization`, `YourProject`, and `YourRepo` with your actual Azure DevOps organization name, project name, and repository name, respectively.
- `token`: Instead of using a username and password combination, Azure DevOps typically requires a Personal Access Token (PAT) for authentication.
 Replace `<your-personal-access-token>` with your actual PAT.

Ensure that you have created a Personal Access Token in your Azure DevOps account with the appropriate permissions to access the repository.
 The token should have at least "Read" permissions if you only intend to pull code from the repository, or "Code (Read & Write)" permissions
  if you also intend to push changes back to the repository.

After updating the Secret manifest, you can apply it to your Kubernetes cluster using `kubectl apply -f <filename.yaml>`.
This will create or update the Secret in the specified namespace (`argocd` in this case), allowing ArgoCD to access your Azure DevOps repository using the provided credentials.

For github use:
apiVersion: v1
kind: Secret
metadata:
  name: argocd-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository #targeting repository..
stringData:
  type: git
  url: https://github.com/yourrepo.git  #url of your github repo
  password: <your-token>
  username: yourusername
  ##NOTE IF YOUR REPOSITORY IS NOT A PUBLIC REPO, THEN YOU HAVE TO AUTHORIZE ARGOCD
      #TO DEPLOY THE APPLICATION USING THE FOLLOWING MANIFEST FILE DEPLOYED IN YOUR K8S CLUSTER.
----------------
You can create a PAT and add repo that you want argocd to read from the settings in argocd UI
In azure for instance, sample repo:
https://binxingnigltd@dev.azure.com/binxingnigltd/nikesales/_git/nikesales
where binxingnigltd is the org name, remove it and add PAT example
https://<PAT>@dev.azure.com/binxingnigltd/nikesales/_git/nikesales
then connect the repo for it to show successful.
Then create application on the UI.
click on application on argocd ui and enter the required data.

Important:
  Azure container registry dockerRegistryServiceConnection.
  To get this,
  After adding a "Docker Registry" service connection, select it from the list of service connections. The url will look something like:

https://your-org.visualstudio.com/YourProject/_settings/adminservices?resourceId=0b6c0705-9fbb-40ec-b629-95cd92856257

Take the resourceId from the query string, which is "0b6c0705-9fbb-40ec-b629-95cd92856257" in the above example, and use it as the dockerRegistryServiceConnection value.
Reference:
https://stackoverflow.com/questions/59439705/what-is-a-docker-registry-service-connection

Important:
  As part of the stages in building the pipeline, we include update image in which we will run a shell script that will update our image tags in a target directory.

  #!/bin/bash

set -x

# Set the repository URL
REPO_URL="https://<ACCESS-TOKEN>@dev.azure.com/<AZURE-DEVOPS-ORG-NAME>/voting-app/_git/voting-app"

# Clone the git repository into the /tmp directory
git clone "$REPO_URL" /tmp/temp_repo

# Navigate into the cloned repository directory
cd /tmp/temp_repo

# Make changes to the Kubernetes manifest file(s)
# For example, let's say you want to change the image tag in a deployment.yaml file
sed -i "s|image:.*|image: <ACR-REGISTRY-NAME>/$2:$3|g" k8s-specifications/$1-deployment.yaml

# Add the modified files
git add .

# Commit the changes
git commit -m "Update Kubernetes manifest"

# Push the changes back to the repository
git push

# Cleanup: remove the temporary directory
rm -rf /tmp/temp_repo

<<<<<<< HEAD
<<<<<<< HEAD
Notes on Azure
Virtual Machine types:
Reference: https://azure.microsoft.com/en-in/pricing/details/virtual-machines/series/
Create a vm depending on your workload requirement and choose configure access to the vm by using ASG and NSG.

AZURE VNET:
ASG: This is used to group diff application or vm to a consolidated set of security group.
It enhances NSG, you'll have to use ASG in collaboration with NSG. ASG will select all vm and you can use NSG to set traffic flow from ASG security group.


**Azure Virtual Network (VNet):**
A VNet is an isolated network within the Azure cloud that allows various Azure resources, such as VMs (Virtual Machines), to securely communicate with each other,
the internet, and on-premises networks. It's the fundamental building block for your private network in Azure.

**Network Security Groups (NSG):**
NSGs are used to filter network traffic to and from Azure resources in an Azure VNet. An NSG can contain multiple inbound and outbound security rules that allow
or deny traffic based on several parameters, such as protocol, port, and source or destination IP address. NSGs can be associated with subnets
in the VNet or directly to individual resources.

**Application Security Groups (ASG):**
ASGs are a feature that enhances the capabilities of NSGs, allowing you to define fine-grained network security policies based on workloads or applications.
 Instead of defining rules based on IP addresses, you can group virtual machines and define network security policies based on these groups.
 This abstraction simplifies the management and maintenance of network security rules, as you don't need to update rules when adding or removing VMs in your application group.

**How ASGs Enhance NSGs:**
ASGs are used in conjunction with NSGs to refine how security rules are applied. When you associate a VM to an ASG,
you can then use that ASG as a source or destination in NSG security rules. This setup allows for more granular control over
traffic flow between different groups of VMs or applications within your Azure VNet. For instance, you can easily allow communication
between your front-end and back-end layers without having to specify individual IP addresses.

By utilizing ASGs with NSGs, you can:
- Dynamically manage security policies as you scale your environment up or down.
- Apply and maintain security rules more efficiently by grouping VMs according to their roles, applications, or workloads.
- Improve the clarity of your security rules, making it easier to understand and audit the traffic flow within your VNet.

In summary, ASGs help organize your virtual machines into logical groups that can be referenced in NSG rules, simplifying the management
and application of network security policies in a dynamic cloud environment.



Important:
  A firewall and a Web Application Firewall (WAF) serve different purposes, and in the context of Azure, they are distinct offerings designed
  for different security needs.

### Azure Firewall

Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It's a stateful firewall
as a service with built-in high availability and

unrestricted cloud scalability. Azure Firewall provides outbound, inbound, and network-level traffic filtering and monitoring.
 It can filter traffic between resources in a VNet, from the internet to the VNet, and even between different VNets. Key features include:
- **FQDN filtering in network rules**
- **Network traffic filtering rules**
- **SNAT support for outbound internet connectivity**
- **Integrated threat intelligence based on Microsoft Threat Intelligence**
- **High availability built-in**
- **Unrestricted cloud scalability**

### Azure Web Application Firewall (WAF)

Azure Web Application Firewall (WAF) is a specialized form of firewall focused on securing web applications. It is offered as part of Azure Application Gateway,
 Azure Front Door Service, and Azure Content Delivery Network (CDN) services. WAF is designed to protect web applications from common exploits and vulnerabilities,
 such as SQL injection, cross-site scripting (XSS), and other web-based attacks. WAF operates at Layer 7 (HTTP/HTTPS)
 and uses rules from the OWASP (Open Web Application Security Project) core rule sets to identify and block malicious traffic. Key features include:
- **Protection against web vulnerabilities and attacks**
- **Custom rules and managed rule sets**
- **Monitoring and logging capabilities**
- **Integration with Azure Application Gateway, Front Door, and CDN**

### Key Differences

- **Scope of Protection**: Azure Firewall provides broad network-level protection and monitoring for resources within
Azure Virtual Networks, including filtering outbound, inbound, and internal network traffic. On the other hand, Azure WAF
 is specifically designed to protect web applications from common web vulnerabilities and attacks.
- **Layer of Operation**: Azure Firewall operates at the network layer (Layers 3 and 4), while Azure WAF operates at the
application layer (Layer 7) to inspect HTTP/HTTPS traffic.
- **Use Cases**: Use Azure Firewall when you need general network security and traffic filtering for all types of resources in your VNet.
 Choose Azure WAF when you specifically need to protect web applications from attacks and vulnerabilities.

In summary, while both services provide security features, they cater to different aspects of network and application security within Azure.
It's not uncommon for organizations to use both Azure Firewall and Azure WAF together to achieve comprehensive security coverage for their cloud resources and applications.

Configure DNAT rule on azure:
to allow traffic from source * or my ip,  destination ip address <firewall public ip> protocol <tcp> destination port <4000>  translated type <ip address> translated address <private ip of the vm>

AZURE STORAGE:
NB: Blob storage --> This also means containers

 Automate Azure Resources using Azure CLI:
   Install or upgrade azure cli.
   check current version run; az version
Reference:
  https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-macos#update
  Using the CLI to create resources is prone to error, we can introduce a bit of automation by using the CLI to create diff resources.

Common Azure CLI commands for managing Azure resources:
Reference:
  https://learn.microsoft.com/en-us/azure/virtual-machines/linux/cli-manage
  This ensures automation, reduction of error and efficiency.

  ERROR WHY LOGIN IN TO AZURE:

➜  Azure-series git:(main) ✗ ./create_vm.sh
AADSTS50076: Due to a configuration change made by your administrator, or because you moved to a new location, you must use multi-factor authentication to access '797f4846-ba00-4fd7-ba43-dac1f8f63013'. Trace ID: 3a8be455-0c91-4eb0-95fb-f1d78e1e2f00 Correlation ID: cfc550eb-e6b2-4928-8a22-6dd7135e377e Timestamp: 2024-03-21 16:32:19Z
Interactive authentication is needed. Please run:
az login --scope https://management.core.windows.net//.default

resolved by running:
az login --tenant <tenant-id>
This is b/c mfa is needed, a code will be sent to the email act associated with your azure act.
or run

Interactive authentication is needed. Please run:
az login --scope https://management.core.windows.net//.default
This will resolve the issue..


Manage azure subscription:
To switch between Azure subscriptions using the Azure CLI, you can use the `az account set` command followed by the subscription
ID or name of the subscription you want to switch to. Here's the general syntax:

```
az account set --subscription SUBSCRIPTION_ID_OR_NAME
```

Replace `SUBSCRIPTION_ID_OR_NAME` with either the subscription ID or the name of the subscription you want to switch to.
You can find the subscription ID and names of your subscriptions by running the following command:

```
az account list --output table
```

This command will list all the subscriptions associated with your account along with their IDs and names.
Once you have the subscription ID or name, you can use the `az account set` command to switch to that subscription. Here's an example:

```
az account set --subscription "My Subscription"
```

Replace `"My Subscription"` with the name of the subscription you want to switch to.

After running the `az account set` command, your default subscription will be updated, and subsequent Azure CLI commands will be executed in the context of the newly selected subscription.
---
az account show --output table
This command will display information about the currently selected subscription, including its ID,
name, and other details, in a tabular format. The subscription marked as (current) is the one currently in use.

Create resource Group:
az group create --name myResourceGroup --location eastus
Reference:
  https://learn.microsoft.com/en-us/cli/azure/manage-azure-groups-azure-cli

  To ssh into the machine created from your script.
  click connect on the console and copy the command ex:
  click on Native SSH
  ssh -i ~/.ssh/id_rsa.pem wiz_obi@52.255.174.251
  The user and the ip will be there

ARM template:
az group create --name arm-vscode --location eastus
az deployment group create --resource-group arm-vscode --template-file azuredeploy.json --parameters azuredeploy.parameters.json

Useful links:
  https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/template-tutorial-add-parameters?tabs=azure-cli
  https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/quickstart-create-templates-use-visual-studio-code?tabs=CLI
  https://learn.microsoft.com/en-us/azure/templates/microsoft.storage/storageaccounts?pivots=deployment-language-arm-template

  We have the.
  1. parameters --> repititive names, that can be used accross env
  2. funtions -->
  3. resources  --> service or resource to create
  4. variables --> container to store dynamic values
  5. outputs --> Things you want to get out of the created resource, say public ip of an vm

Azure IAM from Basics
Azure Managed Identities Demo with Microsoft Entra
Authentications: You have users and groups and authorizations we have actions that can be performed by the authenticated entities..
Authorizations we have roles..
When you create a user account with the root user and sign into that new account, you have sucessfully authenticated into the azure account but you
dont have any role yet.. As a result you can perform any action like creating resources under that account.
When you have been assigned a role to create resources then that is authorization.

Important: You can create custom role and privileges and assign it to a user or group. To create a custom role you have to activate microsoft entra id premium p1 or p2.
Group: This is used to assign roles to group of users.
say you have devops and developers group, instead of creating and assign individual roles to users, you can create a group, add roles to the group.
Then add the users to that group and theyll inherit the permission or role associated to that group..
1 Authentications
2 Authorizations
3 roles
4 users
5 groups

Important: How about resources trying to access each other?
Say you have a requirment that a vm needs access to a blob storage..
How do you implement this? We use service principle and managed Identities.
As stated earlier, role is what a user or service can do, action the user can perform on a resource or action a service can perform on another service.
In terms of service to service interactions we use service principle and managed Identities...
We create a service principle or managed Identities for the vm to access data in the blob storage.
This role is assigned to the entity/service that needs to access the service.
Since vm wants to access resources in blob storage service, we then create a role that will enable vm to do this and assign it to the vm..

Important: when a service wants to talk to other service, in azure we use the concept of.
1. service principle --> This is created and managed by the user and access of the service principle rotated by the user.
2. managed Identities --> This azure manages the access rotation once a user creates it.
Summarily ..
grant a role to the vm called managed identity to be able to access blob containers..
NB: We have system assigned managed identity and user assigned managed identity.
When you enable system assigned managed identity, the system automatically assigns the service or resource a role based on the managed identity.
also note that an Object pinciple ID is generated.
Go to the storage account and add role assignment..

A sample script is on day 12.

AZURE DEVOPS:
---------------------------------
Architecture
1. Overview
2. Boards --> A typical example is Jira
3. Repos --> Used for the storage of application code, a source code repository.
4. Pipelines --> CICD section, you define stages, jobs and steps
5. Test Plans --> A central location where your team can coordinate all your manual test activities, track progress, and get critical insights.
 As a user with basic access level, here is how you can get started right away.

6. Artifacts --> Used for the storage of cicd endproduct or built artifact..

Use Azure Pipelines to support the following scenarios:

    Works with any language or platform
    Deploys to different types of targets at the same time
    Integrates with Azure deployments
    Builds on Windows, Linux, or Mac machines
    Integrates with GitHub
    Works with open-source projects

Reference:
  https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops
  https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/yaml-pipeline-editor?view=azure-devops

  Lets deploy the example voting app on azure devops:
  First test the application locally and see that it works b4 putting it to a cicd pipelines.

  1. Create organization,
  2. create project
  3. import your project code from github to azure devops repo.
  4. create a azure container registry to store built images.
  5. create pipelines for the 3 microservice applications, worker, result and vote.
  First complete the ci jobs which is running of unit test, static test, build of image and pushing built image to the image registry.

  Reference:
    https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/containers/build-image?view=azure-devops

    sample yaml file for the cicd pipeline
    # Docker
# Build and push an image to Azure Container Registry
# https://docs.microsoft.com/azure/devops/pipelines/languages/docker

trigger:
- main

resources:
- repo: self

variables:
  # Container registry service connection established during pipeline creation
  dockerRegistryServiceConnection: '51f380a4-6599-4df5-b1cb-8d8880badb14'
  imageRepository: 'nikesales'
  containerRegistry: 'nikesalescicd.azurecr.io'
  dockerfilePath: '$(Build.SourcesDirectory)/result/Dockerfile'
  tag: '$(Build.BuildId)'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

stages:
- stage: Build
  displayName: Build and push stage
  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName) #azure do not provide runner or pool for a free account..
    steps:
    - task: Docker@2
      displayName: Build and push an image to container registry
      inputs:
        command: buildAndPush
        repository: $(imageRepository)
        dockerfile: $(dockerfilePath)
        containerRegistry: $(dockerRegistryServiceConnection)
        tags: |
          $(tag)

Overview,
trigger:
- main
You can also specify on which action "trigger" should the pipeline run?
Since we have 3 microservices, you define that if a change is made to a specific code of one of the microservice that the pipeline should be triggered,
say a change to the vote, worker or result app code, a pipeline run should be triggered.
This way you wont be building the entire microservice code.. If a change got made in the vote, pipelines run same for result and worker.
As you know in some cases changes cant be done in all microservices at once.. Specify the path in the app code that once change is made the pipeline gets triggered....

Demands and capabilities:
Reference:
  https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/pool-demands?view=azure-pipelines

  Important:
  If you created a pool to run you jobs, ensure you register the name of the agent in the pipeline after Referencing it in the pipeline yaml.
  go to settings, enter the vm or pool name and generate a code to run on you vm to enable it authenticate with the pipeline..

  Reference configuration for the pool:
    https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/linux-agent?view=azure-devops
    Important:
    Create a PAT that will be used to configure the pool
    use agent pool server url while authenticating with the vm as
    Server URL
Azure Pipelines: https://dev.azure.com/{your-organization}

Important:
ECR registry, on the azure Devops pipeline, click on settings and click on Service connections --> New connections.
Then click docker registry --> choose either azure or docker at this point ---> for azure choose Authentication Type which can be
1 service principle or managed service Identities.---> Service connection name <this can be any name>.
This is how you authenticate your azure or any registry to your azure pipeline.

Reference:
  https://stackoverflow.com/questions/59439705/what-is-a-docker-registry-service-connection

  CONTINUOS DELIVERY PIPELINE FOR VOTING APP..
  We used azure devops for the CI, we will use argocd for the cd.. It is a Gitops pull mechanism
  We will authenticate argocd with azure devops repo to monitor for changes in our source code and then deploy our app to kubernetes cluster.
  1. create Azure kubernetes cluster.
  2. While creating AKS, we require agentPool, which is vm.. This is b/c AKS is a managed service and to run our workloads, pods, containers, applications we need a vm..
  agentpool in aks is just like nodegroup in eks, theyre all used to group vm to run our k8s workloads..
  3. set the agentpool, min 1 and max of 2 depending on your requirment.. This is b/c we will use azure scaleset where the vm can scale up based on demand.
  Important:
  you can have diff agentpool,
  1. system agentpool
  2. user agentpool
  theyre can be used to manage diff aspect of your workload..

  NB: validating your aks creation, if you run into resource quota issue, change the region..

  Question:
  Why gitops?
  why shell scripts?
  There is another resource in gitops called argo image updater but its not in GA at the moment.
  Shell script is generic and the agent is linux vm reason for shell script..
  This is why we use python/shell script.
  Why gitops?
  Continous reconciliation, argocd watches the repo and make the desired state the actual state of the cluster.
  It has the sourcecode as it sole point of truth. changes made directly to the cluster is reverted.
  2. deploy argocd in the kubernetes cluster.
  3. configure argocd
  4. create update pipeline for the cd jobs, this will monitor the said path in the repo for changes and this then gets deployed into the k8s cluster.
  NB:
    Before proceeding, you have to install azure cli, login to the sub for the azureagent to manage the aks and deploy app see below
  -------


  Reference to argocd docs.
  https://argo-cd.readthedocs.io/en/stable/
  Note theres no direct connection between the ci and the cd with argocd, argocd handles the cd by monitoring changes in our source code repo and make our desired state the actual state
  of the kubernetes cluster..
  AKS command to run in azureagent:
  https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials
  commands:

  1. install azure cli
  https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt
  curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

  2. login to your azure sub
  3. az account set --subscription 223b3de2-144c-49ec-93dc-765b9cebf465
  4. az aks get-credentials --resource-group nike-sales-dev-rg --name nike-demo-aks-cluster --overwrite-existing

  5 install kubectl
 Run: sudo az aks install-cli --client-version=1.27.9
https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli


If your repository is hosted on Azure DevOps instead of GitHub, you would need to adjust the URL and potentially the authentication method in the Secret accordingly.
Azure DevOps uses different URLs and authentication mechanisms compared to GitHub. Here's how you can modify the Secret manifest:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: argocd-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository
stringData:
  type: git
  url: https://dev.azure.com/YourOrganization/YourProject/_git/YourRepo
  token: <your-personal-access-token>
```

Here's what changed:

- `url`: The URL of the Azure DevOps repository.
Replace `YourOrganization`, `YourProject`, and `YourRepo` with your actual Azure DevOps organization name, project name, and repository name, respectively.
- `token`: Instead of using a username and password combination, Azure DevOps typically requires a Personal Access Token (PAT) for authentication.
 Replace `<your-personal-access-token>` with your actual PAT.

Ensure that you have created a Personal Access Token in your Azure DevOps account with the appropriate permissions to access the repository.
 The token should have at least "Read" permissions if you only intend to pull code from the repository, or "Code (Read & Write)" permissions
  if you also intend to push changes back to the repository.

After updating the Secret manifest, you can apply it to your Kubernetes cluster using `kubectl apply -f <filename.yaml>`.
This will create or update the Secret in the specified namespace (`argocd` in this case), allowing ArgoCD to access your Azure DevOps repository using the provided credentials.

For github use:
apiVersion: v1
kind: Secret
metadata:
  name: argocd-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository #targeting repository..
stringData:
  type: git
  url: https://github.com/yourrepo.git  #url of your github repo
  password: <your-token>
  username: yourusername
  ##NOTE IF YOUR REPOSITORY IS NOT A PUBLIC REPO, THEN YOU HAVE TO AUTHORIZE ARGOCD
      #TO DEPLOY THE APPLICATION USING THE FOLLOWING MANIFEST FILE DEPLOYED IN YOUR K8S CLUSTER.
----------------
You can create a PAT and add repo that you want argocd to read from the settings in argocd UI
In azure for instance, sample repo:
https://binxingnigltd@dev.azure.com/binxingnigltd/nikesales/_git/nikesales
where binxingnigltd is the org name, remove it and add PAT example
https://<PAT>@dev.azure.com/binxingnigltd/nikesales/_git/nikesales
then connect the repo for it to show successful.
Then create application on the UI.
click on application on argocd ui and enter the required data.

Important:
  Azure container registry dockerRegistryServiceConnection.
  To get this,
  After adding a "Docker Registry" service connection, select it from the list of service connections. The url will look something like:

https://your-org.visualstudio.com/YourProject/_settings/adminservices?resourceId=0b6c0705-9fbb-40ec-b629-95cd92856257

Take the resourceId from the query string, which is "0b6c0705-9fbb-40ec-b629-95cd92856257" in the above example, and use it as the dockerRegistryServiceConnection value.
Reference:
https://stackoverflow.com/questions/59439705/what-is-a-docker-registry-service-connection

Important:
  As part of the stages in building the pipeline, we include update image in which we will run a shell script that will update our image tags in a target directory.

  #!/bin/bash

set -x

# Set the repository URL
REPO_URL="https://<ACCESS-TOKEN>@dev.azure.com/<AZURE-DEVOPS-ORG-NAME>/voting-app/_git/voting-app"

# Clone the git repository into the /tmp directory
git clone "$REPO_URL" /tmp/temp_repo

# Navigate into the cloned repository directory
cd /tmp/temp_repo

# Make changes to the Kubernetes manifest file(s)
# For example, let's say you want to change the image tag in a deployment.yaml file
sed -i "s|image:.*|image: <ACR-REGISTRY-NAME>/$2:$3|g" k8s-specifications/$1-deployment.yaml

# Add the modified files
git add .

# Commit the changes
git commit -m "Update Kubernetes manifest"

# Push the changes back to the repository
git push

# Cleanup: remove the temporary directory
rm -rf /tmp/temp_repo

imagepullSecret:
  kubectl create secret docker-registry azurereg \
    --namespace default \
    --docker-server=<> \
    --docker-username=<> \
    --docker-password=<>
Go to the storage account and add role assignment..

Important:
Open port 30000-32676 for the nodePort in the nsg for your AKS.
type vmss --> virtual machine scale set and set the nsg rule by clickng on instance and network and inbound rule...


Important:
 Let's break down each part of the script to understand its purpose and how it operates:

1. **Shebang and Set Debug Mode**:
   ```bash
   #!/bin/bash

   set -x
   ```
   - The shebang `#!/bin/bash` at the beginning specifies that the script should be executed using Bash.
   - `set -x` turns on debugging mode, which causes Bash to display each command it executes. This is useful for understanding the flow of the script and debugging any issues.

2. **Set Repository URL**:
   ```bash
   REPO_URL="https://<ACCESS-TOKEN>@dev.azure.com/<AZURE-DEVOPS-ORG-NAME>/voting-app/_git/voting-app"
   ```
   - This line sets the URL of the Azure DevOps Git repository. You need to replace `<ACCESS-TOKEN>` and `<AZURE-DEVOPS-ORG-NAME>` with your actual access token and organization name, respectively.

3. **Clone the Git Repository**:
   ```bash
   git clone "$REPO_URL" /tmp/temp_repo
   ```
   - This command clones the Git repository specified by `REPO_URL` into the `/tmp/temp_repo` directory.

4. **Navigate into the Cloned Repository Directory**:
   ```bash
   cd /tmp/temp_repo
   ```
   - This command changes the current working directory to the directory where the repository was cloned (`/tmp/temp_repo`).

5. **Make Changes to Kubernetes Manifest File(s)**:
   ```bash
   sed -i "s|image:.*|image: <ACR-REGISTRY-NAME>/$2:$3|g" k8s-specifications/$1-deployment.yaml
   ```
   - This command uses `sed` (stream editor) to replace occurrences of the image tag in the Kubernetes deployment manifest file (`$1-deployment.yaml`).
   - The placeholder `$1` represents the deployment name, `$2` represents the ACR registry name, and `$3` represents the image tag. They are replaced accordingly.

6. **Add Modified Files**:
   ```bash
   git add .
   ```
   - This command stages all modified files in the repository for the next commit.

7. **Commit the Changes**:
   ```bash
   git commit -m "Update Kubernetes manifest"
   ```
   - This command commits the staged changes with the commit message "Update Kubernetes manifest".

8. **Push Changes to Repository**:
   ```bash
   git push
   ```
   - This command pushes the committed changes to the remote repository.

9. **Cleanup: Remove Temporary Directory**:
   ```bash
   rm -rf /tmp/temp_repo
   ```
   - This command removes the temporary directory `/tmp/temp_repo`, which was used for cloning the repository and making changes.

That's the breakdown of the script. It automates the process of updating Kubernetes manifest files in an Azure DevOps Git repository, making it convenient for deployment workflows.

NB: I made changes to the app.py to indicate summer vs winter.. as a developer..


 AKS vs Self Managed Kubernetes Clusters:
--------------------------------------------
We have 3 ways in which we can create our k8s cluster.
1. Azure managed Kubernetes Cluster.
2. Self managed kubernetes cluster using vm created in Azure cloud.
3. Self managed kubernetes cluster created in on-premise data center.
What option is best?

NOTE THESE FACTS:
----------------------
Design and Install a Kubernetes Cluster::;
Before you design a cluster, you must ask relevant questions..

1. Purpose
a. Education
Use--> Minikube
Single node cluster with kubeadm/GCP/AWS

b Development and testing
Multi-node cluster with a single master and multi workers.
setup using kubeadm/GCP/AWS/AKS

c. Hosting production applications
High avai. multi node cluster with multi master node.
setup using kubeadm/GCP/AWS/AKS/kops or other supported platform
upto 5000 nodes
upto 150,000 pods per cluster
upto 300,000 total containers
upto 100 pods per node.

2. Cloud or Onprem
3. Workloads
a. How many application are going to be hosted on the cluster
b. What kind of application will be hosted on the cluster.
i. Web application
ii. Big Data/Analytics

4. Application Resource Requirments
  i. CPU intensive
  ii. Memory intensive

  5. Traffic
  i. Heavy traffic
  ii. Burst traffic





  Choosing Kubernetes Infrastructure:
  turnkey solutions:
  1. vagrant
  2. vmware cloud pks
  3 openshift
  4. cloud foundry cr

  hosted solutions:
  1 GKE
  2 EKS
  3. AKS
  4. OPENSHIFT ONLINE

Deploy an E-Commerce Project on Azure Kubernetes Service:
This is an e-commerce application, we have to first understand the architecture of the app.
Stan's Robot Shop is a sample microservice application you can use as a sandbox to test and learn containerised application orchestration and monitoring techniques.
It is not intended to be a comprehensive reference example of how to write a microservices application,
although you will better understand some of those concepts by playing with Stan's Robot Shop.
To be clear, the error handling is patchy and there is not any security built into the application.

The list of microservices that makes up the stans Robot shop.
1. User microservice --> Takes care of the user registration.
Once the users registers, there is a validation to check the email and the user details is recorded in the database.
The user then logs in with his registered credential, which is validated in the database.
so there is a db that records the user input from the user microservice.

2. catalouge microservice: Within this catalouge theres an image etc

3. cart microservice: say you like any of the robots in the catalouge, you can select it and add it to the cart.
even though i login or logout the information has to be preserved in the cart.
when i checkout, it will no longer be in the cart but the purchased item..
4. payment microservice: This microservice handles the payment, once you go to the cart and finish it, youll be redirected to the payment microservice..

5. shiiping microservice: This calculates the distance the item will be shipped based on the address and the amount

6. dispatch microservice: Once payment is confirmed a item will be shipped..

7. web microservice: This can be nginx or apache for deploying the app.
8. ratings microservice: gives you ratings relating to each robot or item..

Important:
-------------
The user microservice, the catalouge uses mongodb db
The cart microservice uses redis for in Memory
the entire project uses mysql to store images.

The use of the various db solution is to give you a better experience on how these dbs work.
why will one choose redis db against mongodb or mysql: This can be due to application requirements, say we have a ratings application.
you understand that ratings changes, you can check a product with 100 ratings and a few min later its 200 theyre dynamic..
using redis which is an in memory db or caching mechanism is ideal b/c redis is fast in retreiving informations.
if you put those ratings in a mysql or mongodb db, there will be latency in retreiving it.
but with redis which is an in memory db it will be faster to retreive and compute data.

What will happen if the in memory db pod like redis goes down? In k8s we have pv and pvc, that will hold the data and once another redis pods is redeployed it will access the
the data in the pv and be in sync.,.

when should i go for azure files or azure disk?
1. azure disk: if your storage is accessed by a single pod use azure disk which is ebs in aws. with accessmode as readWriteOnce
2. azure files: if your storage is accessed by multiple pods in diff nodes, use azure files.. nfs in aws and acessmode should be readWriteMany

Sometimes you might want to use external storage solutions like
1. Netapp etc, then you will have to create csi driver which will enable you connect and use those external storage solutions..

Important:
  In AKS you can enable an ingress controller,
  Click on the cluster ---> networking and select enable ingress controller..
  It is very easy on aks but in aws you will have to deploy the ingress controller and configure it..
  Say in eks you have to deploy csi drivers but these things come inbuilt in azure kubernetes cluster.
  for aks the ingress controller name or ingressClassName is azure-application-gateway

  Configuration of the application..
  1. Create resource group
  2. create vm for the runner and install dependencies
  3. create k8s cluster

  ISSUES WITH DEPLOYMENT:
  https://github.com/helm/helm/issues/7264
  The answer i got:
  Check the chart for any stray files that may have crept in there. Usually this can occur because there may be some files that are being included as part of the chart.
  my answer: That was exactly the problem.. I had so tar files in the directly containing the chart.yaml.

  Azure DevOps Scenario
  -------------------------

Creating Effective Monitoring System on Azure, Monitor, Prometheus, Network Watcher
What exactly does a Devops or sre engineer monitor?
do they just monitor cpu or memory utilization by your application on a node? no.
for an end user to access an application there are diff chains or component that it has to access to get a response or reach the application.
these various aspect of our app has to be monitored for optimal performance. This way you will build an effective monitoring tools..
1. you have the vnet and its components such as firewall,subnet,nsg etc.
2. The cluster and its components, the apiserver,etcd,scheduler,kube-controller-manager, the worker node component and the applications running in them.
3. storage solutions such as blob,azure disk,azure files etc.
4. vm, serverless fxs and lots of more resources..

it is my responsibility as a devops engineer to setup these network or infrastructure and monitor them.
build an effective monitoring framework to monitor the built infrastructure and it components or services.

important:
monitoring is divided into 6 layers..
1. network traffic or components monitoring: if the network component are not responsive or working as intended, users will not be able to access the application in the cluster.
this results in bad user experience.
2. nodes and node pools: in aks the nodepools are created using vm scale sets. monitoring of the node is essential.
check if the vmss is functioning accordingly. are the node reaching 100% cpu/memory useage.. if this is the case your node wont work as intended bc it has reahed it max capacity.

for 1 and 2 above we collect these metrics and perform analysis and send an alert if the components are not performing well.
3. controlePlane: Monitoring of controlplane component is very important, you might say aks manages that and therefor it must be ok.
When there is an issue with the apiserver, etcd and scheduler etc.. There will be a bad user experience. This is why you should monitor those components..

4. The dataplane component: You can monitor the pods/containers to ensure that it is working as expected.

5. APM Monitoring: application performance monitoring, if this is not monitored it will result in bad user experience.
say your application is not responding to 100s of request even though the application gateway is working properly, it will result to bad user experience.

6. vm, storage accounts and serverless fxns monitoring:

Important:
what are the tools available for monitoring?
Can i use a single tool for all of them? the answer is yes but that is not advisable...
The tool you might be using to monitor application performance might be working but it might not be the best tool...
It is best to pick the tool that is best for each level of monitoring..

THINGS TO MONITOR AND RECOMMENDED TOOLS FOR THE MONITORING
1. network traffic or components monitoring: use Network watcher in azure, This can monitor NSG, vnet, ip flow vpn, firewall, WAF
2.  nodes and node pools: Say cpu, memory use Prometheus.. azure has managed prometheus by azure.

3 controlePlane: use azure monitor,
4. The dataplane component: container/pods use prometheus
5. APM Monitoring: use application insight which is part of azure-monitor..

6. vm, storage accounts and serverless fxns monitoring: use azure monitor, in azure monitor you will see storage act and other services simply link it for monitoring.
also integrate your vm etc..
