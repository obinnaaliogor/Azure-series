Notes on Azure
Virtual Machine types:
Reference: https://azure.microsoft.com/en-in/pricing/details/virtual-machines/series/
Create a vm depending on your workload requirement and choose configure access to the vm by using ASG and NSG.

AZURE VNET:
ASG: This is used to group diff application or vm to a consolidated set of security group.
It enhances NSG, you'll have to use ASG in collaboration with NSG. ASG will select all vm and you can use NSG to set traffic flow from ASG security group.


**Azure Virtual Network (VNet):**
A VNet is an isolated network within the Azure cloud that allows various Azure resources, such as VMs (Virtual Machines), to securely communicate with each other,
the internet, and on-premises networks. It's the fundamental building block for your private network in Azure.

**Network Security Groups (NSG):**
NSGs are used to filter network traffic to and from Azure resources in an Azure VNet. An NSG can contain multiple inbound and outbound security rules that allow
or deny traffic based on several parameters, such as protocol, port, and source or destination IP address. NSGs can be associated with subnets
in the VNet or directly to individual resources.

**Application Security Groups (ASG):**
ASGs are a feature that enhances the capabilities of NSGs, allowing you to define fine-grained network security policies based on workloads or applications.
 Instead of defining rules based on IP addresses, you can group virtual machines and define network security policies based on these groups.
 This abstraction simplifies the management and maintenance of network security rules, as you don't need to update rules when adding or removing VMs in your application group.

**How ASGs Enhance NSGs:**
ASGs are used in conjunction with NSGs to refine how security rules are applied. When you associate a VM to an ASG,
you can then use that ASG as a source or destination in NSG security rules. This setup allows for more granular control over
traffic flow between different groups of VMs or applications within your Azure VNet. For instance, you can easily allow communication
between your front-end and back-end layers without having to specify individual IP addresses.

By utilizing ASGs with NSGs, you can:
- Dynamically manage security policies as you scale your environment up or down.
- Apply and maintain security rules more efficiently by grouping VMs according to their roles, applications, or workloads.
- Improve the clarity of your security rules, making it easier to understand and audit the traffic flow within your VNet.

In summary, ASGs help organize your virtual machines into logical groups that can be referenced in NSG rules, simplifying the management
and application of network security policies in a dynamic cloud environment.



Important:
  A firewall and a Web Application Firewall (WAF) serve different purposes, and in the context of Azure, they are distinct offerings designed
  for different security needs.

### Azure Firewall

Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It's a stateful firewall
as a service with built-in high availability and

unrestricted cloud scalability. Azure Firewall provides outbound, inbound, and network-level traffic filtering and monitoring.
 It can filter traffic between resources in a VNet, from the internet to the VNet, and even between different VNets. Key features include:
- **FQDN filtering in network rules**
- **Network traffic filtering rules**
- **SNAT support for outbound internet connectivity**
- **Integrated threat intelligence based on Microsoft Threat Intelligence**
- **High availability built-in**
- **Unrestricted cloud scalability**

### Azure Web Application Firewall (WAF)

Azure Web Application Firewall (WAF) is a specialized form of firewall focused on securing web applications. It is offered as part of Azure Application Gateway,
 Azure Front Door Service, and Azure Content Delivery Network (CDN) services. WAF is designed to protect web applications from common exploits and vulnerabilities,
 such as SQL injection, cross-site scripting (XSS), and other web-based attacks. WAF operates at Layer 7 (HTTP/HTTPS)
 and uses rules from the OWASP (Open Web Application Security Project) core rule sets to identify and block malicious traffic. Key features include:
- **Protection against web vulnerabilities and attacks**
- **Custom rules and managed rule sets**
- **Monitoring and logging capabilities**
- **Integration with Azure Application Gateway, Front Door, and CDN**

### Key Differences

- **Scope of Protection**: Azure Firewall provides broad network-level protection and monitoring for resources within
Azure Virtual Networks, including filtering outbound, inbound, and internal network traffic. On the other hand, Azure WAF
 is specifically designed to protect web applications from common web vulnerabilities and attacks.
- **Layer of Operation**: Azure Firewall operates at the network layer (Layers 3 and 4), while Azure WAF operates at the
application layer (Layer 7) to inspect HTTP/HTTPS traffic.
- **Use Cases**: Use Azure Firewall when you need general network security and traffic filtering for all types of resources in your VNet.
 Choose Azure WAF when you specifically need to protect web applications from attacks and vulnerabilities.

In summary, while both services provide security features, they cater to different aspects of network and application security within Azure.
It's not uncommon for organizations to use both Azure Firewall and Azure WAF together to achieve comprehensive security coverage for their cloud resources and applications.

Configure DNAT rule on azure:
to allow traffic from source * or my ip,  destination ip address <firewall public ip> protocol <tcp> destination port <4000>  translated type <ip address> translated address <private ip of the vm>

AZURE STORAGE:
NB: Blob storage --> This also means containers

 Automate Azure Resources using Azure CLI:
   Install or upgrade azure cli.
   check current version run; az version
Reference:
  https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-macos#update
  Using the CLI to create resources is prone to error, we can introduce a bit of automation by using the CLI to create diff resources.

Common Azure CLI commands for managing Azure resources:
Reference:
  https://learn.microsoft.com/en-us/azure/virtual-machines/linux/cli-manage
  This ensures automation, reduction of error and efficiency.

  ERROR WHY LOGIN IN TO AZURE:

➜  Azure-series git:(main) ✗ ./create_vm.sh
AADSTS50076: Due to a configuration change made by your administrator, or because you moved to a new location, you must use multi-factor authentication to access '797f4846-ba00-4fd7-ba43-dac1f8f63013'. Trace ID: 3a8be455-0c91-4eb0-95fb-f1d78e1e2f00 Correlation ID: cfc550eb-e6b2-4928-8a22-6dd7135e377e Timestamp: 2024-03-21 16:32:19Z
Interactive authentication is needed. Please run:
az login --scope https://management.core.windows.net//.default

resolved by running:
az login --tenant <tenant-id>
This is b/c mfa is needed, a code will be sent to the email act associated with your azure act.
or run

Interactive authentication is needed. Please run:
az login --scope https://management.core.windows.net//.default
This will resolve the issue..


Manage azure subscription:
To switch between Azure subscriptions using the Azure CLI, you can use the `az account set` command followed by the subscription
ID or name of the subscription you want to switch to. Here's the general syntax:

```
az account set --subscription SUBSCRIPTION_ID_OR_NAME
```

Replace `SUBSCRIPTION_ID_OR_NAME` with either the subscription ID or the name of the subscription you want to switch to.
You can find the subscription ID and names of your subscriptions by running the following command:

```
az account list --output table
```

This command will list all the subscriptions associated with your account along with their IDs and names.
Once you have the subscription ID or name, you can use the `az account set` command to switch to that subscription. Here's an example:

```
az account set --subscription "My Subscription"
```

Replace `"My Subscription"` with the name of the subscription you want to switch to.

After running the `az account set` command, your default subscription will be updated, and subsequent Azure CLI commands will be executed in the context of the newly selected subscription.
---
az account show --output table
This command will display information about the currently selected subscription, including its ID,
name, and other details, in a tabular format. The subscription marked as (current) is the one currently in use.

Create resource Group:
az group create --name myResourceGroup --location eastus
Reference:
  https://learn.microsoft.com/en-us/cli/azure/manage-azure-groups-azure-cli

  To ssh into the machine created from your script.
  click connect on the console and copy the command ex:
  click on Native SSH
  ssh -i ~/.ssh/id_rsa.pem wiz_obi@52.255.174.251
  The user and the ip will be there

ARM template:
az group create --name arm-vscode --location eastus
az deployment group create --resource-group arm-vscode --template-file azuredeploy.json --parameters azuredeploy.parameters.json

Useful links:
  https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/template-tutorial-add-parameters?tabs=azure-cli
  https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/quickstart-create-templates-use-visual-studio-code?tabs=CLI
  https://learn.microsoft.com/en-us/azure/templates/microsoft.storage/storageaccounts?pivots=deployment-language-arm-template

  We have the.
  1. parameters --> repititive names, that can be used accross env
  2. funtions -->
  3. resources  --> service or resource to create
  4. variables --> container to store dynamic values
  5. outputs --> Things you want to get out of the created resource, say public ip of an vm

Azure IAM from Basics
Azure Managed Identities Demo with Microsoft Entra
Authentications: You have users and groups and authorizations we have actions that can be performed by the authenticated entities..
Authorizations we have roles..
When you create a user account with the root user and sign into that new account, you have sucessfully authenticated into the azure account but you
dont have any role yet.. As a result you can perform any action like creating resources under that account.
When you have been assigned a role to create resources then that is authorization.

Important: You can create custom role and privileges and assign it to a user or group. To create a custom role you have to activate microsoft entra id premium p1 or p2.
Group: This is used to assign roles to group of users.
say you have devops and developers group, instead of creating and assign individual roles to users, you can create a group, add roles to the group.
Then add the users to that group and theyll inherit the permission or role associated to that group..
1 Authentications
2 Authorizations
3 roles
4 users
5 groups

Important: How about resources trying to access each other?
Say you have a requirment that a vm needs access to a blob storage..
How do you implement this? We use service principle and managed Identities.
As stated earlier, role is what a user or service can do, action the user can perform on a resource or action a service can perform on another service.
In terms of service to service interactions we use service principle and managed Identities...
We create a service principle or managed Identities for the vm to access data in the blob storage.
This role is assigned to the entity/service that needs to access the service.
Since vm wants to access resources in blob storage service, we then create a role that will enable vm to do this and assign it to the vm..

Important: when a service wants to talk to other service, in azure we use the concept of.
1. service principle --> This is created and managed by the user and access of the service principle is rotated by the user.
2. managed Identities --> This, azure manages the access rotation once a user creates it.
Summarily ..
grant a role to the vm called managed identity to be able to access blob containers..
NB: We have system assigned managed identity and user assigned managed identity.
When you enable system assigned managed identity, the system automatically assigns the service or resource a role based on the managed identity.
also note that an Object pinciple ID is generated.
<<<<<<< HEAD
Go to the storage account and add role assignment..

A sample script is on day 12.

AZURE DEVOPS:
---------------------------------
Architecture
1. Overview
2. Boards --> A typical example is Jira
3. Repos --> Used for the storage of application code, a source code repository.
4. Pipelines --> CICD section, you define stages, jobs and steps
5. Test Plans --> A central location where your team can coordinate all your manual test activities, track progress, and get critical insights.
 As a user with basic access level, here is how you can get started right away.

6. Artifacts --> Used for the storage of cicd endproduct or built artifact..

Use Azure Pipelines to support the following scenarios:

    Works with any language or platform
    Deploys to different types of targets at the same time
    Integrates with Azure deployments
    Builds on Windows, Linux, or Mac machines
    Integrates with GitHub
    Works with open-source projects

Reference:
  https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops
  https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/yaml-pipeline-editor?view=azure-devops

  Lets deploy the example voting app on azure devops:
  First test the application locally and see that it works b4 putting it to a cicd pipelines.

  1. Create organization,
  2. create project
  3. import your project code from github to azure devops repo.
  4. create a azure container registry to store built images.
  5. create pipelines for the 3 microservice applications, worker, result and vote.
  First complete the ci jobs which is running of unit test, static test, build of image and pushing built image to the image registry.

  Reference:
    https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/containers/build-image?view=azure-devops

    sample yaml file for the cicd pipeline
    # Docker
# Build and push an image to Azure Container Registry
# https://docs.microsoft.com/azure/devops/pipelines/languages/docker

trigger:
- main

resources:
- repo: self

variables:
  # Container registry service connection established during pipeline creation
  dockerRegistryServiceConnection: '51f380a4-6599-4df5-b1cb-8d8880badb14'
  imageRepository: 'nikesales'
  containerRegistry: 'nikesalescicd.azurecr.io'
  dockerfilePath: '$(Build.SourcesDirectory)/result/Dockerfile'
  tag: '$(Build.BuildId)'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

stages:
- stage: Build
  displayName: Build and push stage
  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName) #azure do not provide runner or pool for a free account..
    steps:
    - task: Docker@2
      displayName: Build and push an image to container registry
      inputs:
        command: buildAndPush
        repository: $(imageRepository)
        dockerfile: $(dockerfilePath)
        containerRegistry: $(dockerRegistryServiceConnection)
        tags: |
          $(tag)

Overview,
trigger:
- main
You can also specify on which action "trigger" should the pipeline run?
Since we have 3 microservices, you define that if a change is made to a specific code of one of the microservice that the pipeline should be triggered,
say a change to the vote, worker or result app code, a pipeline run should be triggered.
This way you wont be building the entire microservice code.. If a change got made in the vote, pipelines run same for result and worker.
As you know in some cases changes cant be done in all microservices at once.. Specify the path in the app code that once change is made the pipeline gets triggered....

Demands and capabilities:
Reference:
  https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/pool-demands?view=azure-pipelines

  Important:
  If you created a pool to run you jobs, ensure you register the name of the agent in the pipeline after Referencing it in the pipeline yaml.
  go to settings, enter the vm or pool name and generate a code to run on you vm to enable it authenticate with the pipeline..

  Reference configuration for the pool:
    https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/linux-agent?view=azure-devops
    Important:
    Create a PAT that will be used to configure the pool
    use agent pool server url while authenticating with the vm as
    Server URL
Azure Pipelines: https://dev.azure.com/{your-organization}

Important:
ECR registry, on the azure Devops pipeline, click on settings and click on Service connections --> New connections.
Then click docker registry --> choose either azure or docker at this point ---> for azure choose Authentication Type which can be
1 service principle or managed service Identities.---> Service connection name <this can be any name>.
This is how you authenticate your azure or any registry to your azure pipeline.

Reference:
  https://stackoverflow.com/questions/59439705/what-is-a-docker-registry-service-connection

  CONTINUOS DELIVERY PIPELINE FOR VOTING APP..
  We used azure devops for the CI, we will use argocd for the cd.. It is a Gitops pull mechanism
  We will authenticate argocd with azure devops repo to monitor for changes in our source code and then deploy our app to kubernetes cluster.
  1. create Azure kubernetes cluster.
  2. While creating AKS, we require agentPool, which is vm.. This is b/c AKS is a managed service and to run our workloads, pods, containers, applications we need a vm..
  agentpool in aks is just like nodegroup in eks, theyre all used to group vm to run our k8s workloads..
  3. set the agentpool, min 1 and max of 2 depending on your requirment.. This is b/c we will use azure scaleset where the vm can scale up based on demand.
  Important:
  you can have diff agentpool,
  1. system agentpool
  2. user agentpool
  theyre can be used to manage diff aspect of your workload..

  NB: validating your aks creation, if you run into resource quota issue, change the region..

  Question:
  Why gitops?
  why shell scripts?
  There is another resource in gitops called argo image updater but its not in GA at the moment.
  Shell script is generic and the agent is linux vm reason for shell script..
  This is why we use python/shell script.
  Why gitops?
  Continous reconciliation, argocd watches the repo and make the desired state the actual state of the cluster.
  It has the sourcecode as it sole point of truth. changes made directly to the cluster is reverted.
  2. deploy argocd in the kubernetes cluster.
  3. configure argocd
  4. create update pipeline for the cd jobs, this will monitor the said path in the repo for changes and this then gets deployed into the k8s cluster.
  NB:
    Before proceeding, you have to install azure cli, login to the sub for the azureagent to manage the aks and deploy app see below
  -------


  Reference to argocd docs.
  https://argo-cd.readthedocs.io/en/stable/
  Note theres no direct connection between the ci and the cd with argocd, argocd handles the cd by monitoring changes in our source code repo and make our desired state the actual state
  of the kubernetes cluster..
  AKS command to run in azureagent:
  https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials
  commands:

  1. install azure cli
  https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt
  curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

  2. login to your azure sub
  3. az account set --subscription 223b3de2-144c-49ec-93dc-765b9cebf465
  4. az aks get-credentials --resource-group stans-robot-shop --name stans-robot-shop-demo --overwrite-existing

  5 install kubectl
 Run: sudo az aks install-cli --client-version=1.27.9
https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli


If your repository is hosted on Azure DevOps instead of GitHub, you would need to adjust the URL and potentially the authentication method in the Secret accordingly.
Azure DevOps uses different URLs and authentication mechanisms compared to GitHub. Here's how you can modify the Secret manifest:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: argocd-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository
stringData:
  type: git
  url: https://dev.azure.com/YourOrganization/YourProject/_git/YourRepo
  token: <your-personal-access-token>
```

Here's what changed:

- `url`: The URL of the Azure DevOps repository.
Replace `YourOrganization`, `YourProject`, and `YourRepo` with your actual Azure DevOps organization name, project name, and repository name, respectively.
- `token`: Instead of using a username and password combination, Azure DevOps typically requires a Personal Access Token (PAT) for authentication.
 Replace `<your-personal-access-token>` with your actual PAT.

Ensure that you have created a Personal Access Token in your Azure DevOps account with the appropriate permissions to access the repository.
 The token should have at least "Read" permissions if you only intend to pull code from the repository, or "Code (Read & Write)" permissions
  if you also intend to push changes back to the repository.

After updating the Secret manifest, you can apply it to your Kubernetes cluster using `kubectl apply -f <filename.yaml>`.
This will create or update the Secret in the specified namespace (`argocd` in this case), allowing ArgoCD to access your Azure DevOps repository using the provided credentials.

For github use:
apiVersion: v1
kind: Secret
metadata:
  name: argocd-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository #targeting repository..
stringData:
  type: git
  url: https://github.com/yourrepo.git  #url of your github repo
  password: <your-token>
  username: yourusername
  ##NOTE IF YOUR REPOSITORY IS NOT A PUBLIC REPO, THEN YOU HAVE TO AUTHORIZE ARGOCD
      #TO DEPLOY THE APPLICATION USING THE FOLLOWING MANIFEST FILE DEPLOYED IN YOUR K8S CLUSTER.
----------------
You can create a PAT and add repo that you want argocd to read from the settings in argocd UI
In azure for instance, sample repo:
https://binxingnigltd@dev.azure.com/binxingnigltd/nikesales/_git/nikesales
where binxingnigltd is the org name, remove it and add PAT example
https://<PAT>@dev.azure.com/binxingnigltd/nikesales/_git/nikesales
then connect the repo for it to show successful.
Then create application on the UI.
click on application on argocd ui and enter the required data.

Important:
  Azure container registry dockerRegistryServiceConnection.
  To get this,
  After adding a "Docker Registry" service connection, select it from the list of service connections. The url will look something like:

https://your-org.visualstudio.com/YourProject/_settings/adminservices?resourceId=0b6c0705-9fbb-40ec-b629-95cd92856257

Take the resourceId from the query string, which is "0b6c0705-9fbb-40ec-b629-95cd92856257" in the above example, and use it as the dockerRegistryServiceConnection value.
Reference:
https://stackoverflow.com/questions/59439705/what-is-a-docker-registry-service-connection

Important:
  As part of the stages in building the pipeline, we include update image in which we will run a shell script that will update our image tags in a target directory.

  #!/bin/bash

set -x

# Set the repository URL
REPO_URL="https://<ACCESS-TOKEN>@dev.azure.com/<AZURE-DEVOPS-ORG-NAME>/voting-app/_git/voting-app"

# Clone the git repository into the /tmp directory
git clone "$REPO_URL" /tmp/temp_repo

# Navigate into the cloned repository directory
cd /tmp/temp_repo

# Make changes to the Kubernetes manifest file(s)
# For example, let's say you want to change the image tag in a deployment.yaml file
sed -i "s|image:.*|image: <ACR-REGISTRY-NAME>/$2:$3|g" k8s-specifications/$1-deployment.yaml

# Add the modified files
git add .

# Commit the changes
git commit -m "Update Kubernetes manifest"

# Push the changes back to the repository
git push

# Cleanup: remove the temporary directory
rm -rf /tmp/temp_repo

<<<<<<< HEAD
<<<<<<< HEAD
Notes on Azure
Virtual Machine types:
Reference: https://azure.microsoft.com/en-in/pricing/details/virtual-machines/series/
Create a vm depending on your workload requirement and choose configure access to the vm by using ASG and NSG.

AZURE VNET:
ASG: This is used to group diff application or vm to a consolidated set of security group.
It enhances NSG, you'll have to use ASG in collaboration with NSG. ASG will select all vm and you can use NSG to set traffic flow from ASG security group.


**Azure Virtual Network (VNet):**
A VNet is an isolated network within the Azure cloud that allows various Azure resources, such as VMs (Virtual Machines), to securely communicate with each other,
the internet, and on-premises networks. It's the fundamental building block for your private network in Azure.

**Network Security Groups (NSG):**
NSGs are used to filter network traffic to and from Azure resources in an Azure VNet. An NSG can contain multiple inbound and outbound security rules that allow
or deny traffic based on several parameters, such as protocol, port, and source or destination IP address. NSGs can be associated with subnets
in the VNet or directly to individual resources.

**Application Security Groups (ASG):**
ASGs are a feature that enhances the capabilities of NSGs, allowing you to define fine-grained network security policies based on workloads or applications.
 Instead of defining rules based on IP addresses, you can group virtual machines and define network security policies based on these groups.
 This abstraction simplifies the management and maintenance of network security rules, as you don't need to update rules when adding or removing VMs in your application group.

**How ASGs Enhance NSGs:**
ASGs are used in conjunction with NSGs to refine how security rules are applied. When you associate a VM to an ASG,
you can then use that ASG as a source or destination in NSG security rules. This setup allows for more granular control over
traffic flow between different groups of VMs or applications within your Azure VNet. For instance, you can easily allow communication
between your front-end and back-end layers without having to specify individual IP addresses.

By utilizing ASGs with NSGs, you can:
- Dynamically manage security policies as you scale your environment up or down.
- Apply and maintain security rules more efficiently by grouping VMs according to their roles, applications, or workloads.
- Improve the clarity of your security rules, making it easier to understand and audit the traffic flow within your VNet.

In summary, ASGs help organize your virtual machines into logical groups that can be referenced in NSG rules, simplifying the management
and application of network security policies in a dynamic cloud environment.



Important:
  A firewall and a Web Application Firewall (WAF) serve different purposes, and in the context of Azure, they are distinct offerings designed
  for different security needs.

### Azure Firewall

Azure Firewall is a managed, cloud-based network security service that protects your Azure Virtual Network resources. It's a stateful firewall
as a service with built-in high availability and

unrestricted cloud scalability. Azure Firewall provides outbound, inbound, and network-level traffic filtering and monitoring.
 It can filter traffic between resources in a VNet, from the internet to the VNet, and even between different VNets. Key features include:
- **FQDN filtering in network rules**
- **Network traffic filtering rules**
- **SNAT support for outbound internet connectivity**
- **Integrated threat intelligence based on Microsoft Threat Intelligence**
- **High availability built-in**
- **Unrestricted cloud scalability**

### Azure Web Application Firewall (WAF)

Azure Web Application Firewall (WAF) is a specialized form of firewall focused on securing web applications. It is offered as part of Azure Application Gateway,
 Azure Front Door Service, and Azure Content Delivery Network (CDN) services. WAF is designed to protect web applications from common exploits and vulnerabilities,
 such as SQL injection, cross-site scripting (XSS), and other web-based attacks. WAF operates at Layer 7 (HTTP/HTTPS)
 and uses rules from the OWASP (Open Web Application Security Project) core rule sets to identify and block malicious traffic. Key features include:
- **Protection against web vulnerabilities and attacks**
- **Custom rules and managed rule sets**
- **Monitoring and logging capabilities**
- **Integration with Azure Application Gateway, Front Door, and CDN**

### Key Differences

- **Scope of Protection**: Azure Firewall provides broad network-level protection and monitoring for resources within
Azure Virtual Networks, including filtering outbound, inbound, and internal network traffic. On the other hand, Azure WAF
 is specifically designed to protect web applications from common web vulnerabilities and attacks.
- **Layer of Operation**: Azure Firewall operates at the network layer (Layers 3 and 4), while Azure WAF operates at the
application layer (Layer 7) to inspect HTTP/HTTPS traffic.
- **Use Cases**: Use Azure Firewall when you need general network security and traffic filtering for all types of resources in your VNet.
 Choose Azure WAF when you specifically need to protect web applications from attacks and vulnerabilities.

In summary, while both services provide security features, they cater to different aspects of network and application security within Azure.
It's not uncommon for organizations to use both Azure Firewall and Azure WAF together to achieve comprehensive security coverage for their cloud resources and applications.

Configure DNAT rule on azure:
to allow traffic from source * or my ip,  destination ip address <firewall public ip> protocol <tcp> destination port <4000>  translated type <ip address> translated address <private ip of the vm>

AZURE STORAGE:
NB: Blob storage --> This also means containers

 Automate Azure Resources using Azure CLI:
   Install or upgrade azure cli.
   check current version run; az version
Reference:
  https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-macos#update
  Using the CLI to create resources is prone to error, we can introduce a bit of automation by using the CLI to create diff resources.

Common Azure CLI commands for managing Azure resources:
Reference:
  https://learn.microsoft.com/en-us/azure/virtual-machines/linux/cli-manage
  This ensures automation, reduction of error and efficiency.

  ERROR WHY LOGIN IN TO AZURE:

➜  Azure-series git:(main) ✗ ./create_vm.sh
AADSTS50076: Due to a configuration change made by your administrator, or because you moved to a new location, you must use multi-factor authentication to access '797f4846-ba00-4fd7-ba43-dac1f8f63013'. Trace ID: 3a8be455-0c91-4eb0-95fb-f1d78e1e2f00 Correlation ID: cfc550eb-e6b2-4928-8a22-6dd7135e377e Timestamp: 2024-03-21 16:32:19Z
Interactive authentication is needed. Please run:
az login --scope https://management.core.windows.net//.default

resolved by running:
az login --tenant <tenant-id>
This is b/c mfa is needed, a code will be sent to the email act associated with your azure act.
or run

Interactive authentication is needed. Please run:
az login --scope https://management.core.windows.net//.default
This will resolve the issue..


Manage azure subscription:
To switch between Azure subscriptions using the Azure CLI, you can use the `az account set` command followed by the subscription
ID or name of the subscription you want to switch to. Here's the general syntax:

```
az account set --subscription SUBSCRIPTION_ID_OR_NAME
```

Replace `SUBSCRIPTION_ID_OR_NAME` with either the subscription ID or the name of the subscription you want to switch to.
You can find the subscription ID and names of your subscriptions by running the following command:

```
az account list --output table
```

This command will list all the subscriptions associated with your account along with their IDs and names.
Once you have the subscription ID or name, you can use the `az account set` command to switch to that subscription. Here's an example:

```
az account set --subscription "My Subscription"
```

Replace `"My Subscription"` with the name of the subscription you want to switch to.

After running the `az account set` command, your default subscription will be updated, and subsequent Azure CLI commands will be executed in the context of the newly selected subscription.
---
az account show --output table
This command will display information about the currently selected subscription, including its ID,
name, and other details, in a tabular format. The subscription marked as (current) is the one currently in use.

Create resource Group:
az group create --name myResourceGroup --location eastus
Reference:
  https://learn.microsoft.com/en-us/cli/azure/manage-azure-groups-azure-cli

  To ssh into the machine created from your script.
  click connect on the console and copy the command ex:
  click on Native SSH
  ssh -i ~/.ssh/id_rsa.pem wiz_obi@52.255.174.251
  The user and the ip will be there

ARM template:
az group create --name arm-vscode --location eastus
az deployment group create --resource-group arm-vscode --template-file azuredeploy.json --parameters azuredeploy.parameters.json

Useful links:
  https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/template-tutorial-add-parameters?tabs=azure-cli
  https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/quickstart-create-templates-use-visual-studio-code?tabs=CLI
  https://learn.microsoft.com/en-us/azure/templates/microsoft.storage/storageaccounts?pivots=deployment-language-arm-template

  We have the.
  1. parameters --> repititive names, that can be used accross env
  2. funtions -->
  3. resources  --> service or resource to create
  4. variables --> container to store dynamic values
  5. outputs --> Things you want to get out of the created resource, say public ip of an vm

Azure IAM from Basics
Azure Managed Identities Demo with Microsoft Entra
Authentications: You have users and groups and authorizations we have actions that can be performed by the authenticated entities..
Authorizations we have roles..
When you create a user account with the root user and sign into that new account, you have sucessfully authenticated into the azure account but you
dont have any role yet.. As a result you can perform any action like creating resources under that account.
When you have been assigned a role to create resources then that is authorization.

Important: You can create custom role and privileges and assign it to a user or group. To create a custom role you have to activate microsoft entra id premium p1 or p2.
Group: This is used to assign roles to group of users.
say you have devops and developers group, instead of creating and assign individual roles to users, you can create a group, add roles to the group.
Then add the users to that group and theyll inherit the permission or role associated to that group..
1 Authentications
2 Authorizations
3 roles
4 users
5 groups

Important: How about resources trying to access each other?
Say you have a requirment that a vm needs access to a blob storage..
How do you implement this? We use service principle and managed Identities.
As stated earlier, role is what a user or service can do, action the user can perform on a resource or action a service can perform on another service.
In terms of service to service interactions we use service principle and managed Identities...
We create a service principle or managed Identities for the vm to access data in the blob storage.
This role is assigned to the entity/service that needs to access the service.
Since vm wants to access resources in blob storage service, we then create a role that will enable vm to do this and assign it to the vm..

Important: when a service wants to talk to other service, in azure we use the concept of.
1. service principle --> This is created and managed by the user and access of the service principle rotated by the user.
2. managed Identities --> This azure manages the access rotation once a user creates it.
Summarily ..
grant a role to the vm called managed identity to be able to access blob containers..
NB: We have system assigned managed identity and user assigned managed identity.
When you enable system assigned managed identity, the system automatically assigns the service or resource a role based on the managed identity.
also note that an Object pinciple ID is generated.
Go to the storage account and add role assignment..

A sample script is on day 12.

AZURE DEVOPS:
---------------------------------
Architecture
1. Overview
2. Boards --> A typical example is Jira
3. Repos --> Used for the storage of application code, a source code repository.
4. Pipelines --> CICD section, you define stages, jobs and steps
5. Test Plans --> A central location where your team can coordinate all your manual test activities, track progress, and get critical insights.
 As a user with basic access level, here is how you can get started right away.

6. Artifacts --> Used for the storage of cicd endproduct or built artifact..

Use Azure Pipelines to support the following scenarios:

    Works with any language or platform
    Deploys to different types of targets at the same time
    Integrates with Azure deployments
    Builds on Windows, Linux, or Mac machines
    Integrates with GitHub
    Works with open-source projects

Reference:
  https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/what-is-azure-pipelines?view=azure-devops
  https://learn.microsoft.com/en-us/azure/devops/pipelines/get-started/yaml-pipeline-editor?view=azure-devops

  Lets deploy the example voting app on azure devops:
  First test the application locally and see that it works b4 putting it to a cicd pipelines.

  1. Create organization,
  2. create project
  3. import your project code from github to azure devops repo.
  4. create a azure container registry to store built images.
  5. create pipelines for the 3 microservice applications, worker, result and vote.
  First complete the ci jobs which is running of unit test, static test, build of image and pushing built image to the image registry.

  Reference:
    https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/containers/build-image?view=azure-devops

    sample yaml file for the cicd pipeline
    # Docker
# Build and push an image to Azure Container Registry
# https://docs.microsoft.com/azure/devops/pipelines/languages/docker

trigger:
- main

resources:
- repo: self

variables:
  # Container registry service connection established during pipeline creation
  dockerRegistryServiceConnection: '51f380a4-6599-4df5-b1cb-8d8880badb14'
  imageRepository: 'nikesales'
  containerRegistry: 'nikesalescicd.azurecr.io'
  dockerfilePath: '$(Build.SourcesDirectory)/result/Dockerfile'
  tag: '$(Build.BuildId)'

  # Agent VM image name
  vmImageName: 'ubuntu-latest'

stages:
- stage: Build
  displayName: Build and push stage
  jobs:
  - job: Build
    displayName: Build
    pool:
      vmImage: $(vmImageName) #azure do not provide runner or pool for a free account..
    steps:
    - task: Docker@2
      displayName: Build and push an image to container registry
      inputs:
        command: buildAndPush
        repository: $(imageRepository)
        dockerfile: $(dockerfilePath)
        containerRegistry: $(dockerRegistryServiceConnection)
        tags: |
          $(tag)

Overview,
trigger:
- main
You can also specify on which action "trigger" should the pipeline run?
Since we have 3 microservices, you define that if a change is made to a specific code of one of the microservice that the pipeline should be triggered,
say a change to the vote, worker or result app code, a pipeline run should be triggered.
This way you wont be building the entire microservice code.. If a change got made in the vote, pipelines run same for result and worker.
As you know in some cases changes cant be done in all microservices at once.. Specify the path in the app code that once change is made the pipeline gets triggered....

Demands and capabilities:
Reference:
  https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/pool-demands?view=azure-pipelines

  Important:
  If you created a pool to run you jobs, ensure you register the name of the agent in the pipeline after Referencing it in the pipeline yaml.
  go to settings, enter the vm or pool name and generate a code to run on you vm to enable it authenticate with the pipeline..

  Reference configuration for the pool:
    https://learn.microsoft.com/en-us/azure/devops/pipelines/agents/linux-agent?view=azure-devops
    Important:
    Create a PAT that will be used to configure the pool
    use agent pool server url while authenticating with the vm as
    Server URL
Azure Pipelines: https://dev.azure.com/{your-organization}

Important:
ECR registry, on the azure Devops pipeline, click on settings and click on Service connections --> New connections.
Then click docker registry --> choose either azure or docker at this point ---> for azure choose Authentication Type which can be
1 service principle or managed service Identities.---> Service connection name <this can be any name>.
This is how you authenticate your azure or any registry to your azure pipeline.

Reference:
  https://stackoverflow.com/questions/59439705/what-is-a-docker-registry-service-connection

  CONTINUOS DELIVERY PIPELINE FOR VOTING APP..
  We used azure devops for the CI, we will use argocd for the cd.. It is a Gitops pull mechanism
  We will authenticate argocd with azure devops repo to monitor for changes in our source code and then deploy our app to kubernetes cluster.
  1. create Azure kubernetes cluster.
  2. While creating AKS, we require agentPool, which is vm.. This is b/c AKS is a managed service and to run our workloads, pods, containers, applications we need a vm..
  agentpool in aks is just like nodegroup in eks, theyre all used to group vm to run our k8s workloads..
  3. set the agentpool, min 1 and max of 2 depending on your requirment.. This is b/c we will use azure scaleset where the vm can scale up based on demand.
  Important:
  you can have diff agentpool,
  1. system agentpool
  2. user agentpool
  theyre can be used to manage diff aspect of your workload..

  NB: validating your aks creation, if you run into resource quota issue, change the region..

  Question:
  Why gitops?
  why shell scripts?
  There is another resource in gitops called argo image updater but its not in GA at the moment.
  Shell script is generic and the agent is linux vm reason for shell script..
  This is why we use python/shell script.
  Why gitops?
  Continous reconciliation, argocd watches the repo and make the desired state the actual state of the cluster.
  It has the sourcecode as it sole point of truth. changes made directly to the cluster is reverted.
  2. deploy argocd in the kubernetes cluster.
  3. configure argocd
  4. create update pipeline for the cd jobs, this will monitor the said path in the repo for changes and this then gets deployed into the k8s cluster.
  NB:
    Before proceeding, you have to install azure cli, login to the sub for the azureagent to manage the aks and deploy app see below
  -------


  Reference to argocd docs.
  https://argo-cd.readthedocs.io/en/stable/
  Note theres no direct connection between the ci and the cd with argocd, argocd handles the cd by monitoring changes in our source code repo and make our desired state the actual state
  of the kubernetes cluster..
  AKS command to run in azureagent:
  https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-get-credentials
  commands:

  1. install azure cli
  https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-linux?pivots=apt
  curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

  2. login to your azure sub
  3. az account set --subscription 223b3de2-144c-49ec-93dc-765b9cebf465
  4. az aks get-credentials --resource-group nike-sales-dev-rg --name nike-demo-aks-cluster --overwrite-existing

  5 install kubectl
 Run: sudo az aks install-cli --client-version=1.27.9
https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-install-cli


If your repository is hosted on Azure DevOps instead of GitHub, you would need to adjust the URL and potentially the authentication method in the Secret accordingly.
Azure DevOps uses different URLs and authentication mechanisms compared to GitHub. Here's how you can modify the Secret manifest:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: argocd-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository
stringData:
  type: git
  url: https://dev.azure.com/YourOrganization/YourProject/_git/YourRepo
  token: <your-personal-access-token>
```

Here's what changed:

- `url`: The URL of the Azure DevOps repository.
Replace `YourOrganization`, `YourProject`, and `YourRepo` with your actual Azure DevOps organization name, project name, and repository name, respectively.
- `token`: Instead of using a username and password combination, Azure DevOps typically requires a Personal Access Token (PAT) for authentication.
 Replace `<your-personal-access-token>` with your actual PAT.

Ensure that you have created a Personal Access Token in your Azure DevOps account with the appropriate permissions to access the repository.
 The token should have at least "Read" permissions if you only intend to pull code from the repository, or "Code (Read & Write)" permissions
  if you also intend to push changes back to the repository.

After updating the Secret manifest, you can apply it to your Kubernetes cluster using `kubectl apply -f <filename.yaml>`.
This will create or update the Secret in the specified namespace (`argocd` in this case), allowing ArgoCD to access your Azure DevOps repository using the provided credentials.

For github use:
apiVersion: v1
kind: Secret
metadata:
  name: argocd-repo
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: repository #targeting repository..
stringData:
  type: git
  url: https://github.com/yourrepo.git  #url of your github repo
  password: <your-token>
  username: yourusername
  ##NOTE IF YOUR REPOSITORY IS NOT A PUBLIC REPO, THEN YOU HAVE TO AUTHORIZE ARGOCD
      #TO DEPLOY THE APPLICATION USING THE FOLLOWING MANIFEST FILE DEPLOYED IN YOUR K8S CLUSTER.
----------------
You can create a PAT and add repo that you want argocd to read from the settings in argocd UI
In azure for instance, sample repo:
https://binxingnigltd@dev.azure.com/binxingnigltd/nikesales/_git/nikesales
where binxingnigltd is the org name, remove it and add PAT example
https://<PAT>@dev.azure.com/binxingnigltd/nikesales/_git/nikesales
then connect the repo for it to show successful.
Then create application on the UI.
click on application on argocd ui and enter the required data.

Important:
  Azure container registry dockerRegistryServiceConnection.
  To get this,
  After adding a "Docker Registry" service connection, select it from the list of service connections. The url will look something like:

https://your-org.visualstudio.com/YourProject/_settings/adminservices?resourceId=0b6c0705-9fbb-40ec-b629-95cd92856257

Take the resourceId from the query string, which is "0b6c0705-9fbb-40ec-b629-95cd92856257" in the above example, and use it as the dockerRegistryServiceConnection value.
Reference:
https://stackoverflow.com/questions/59439705/what-is-a-docker-registry-service-connection

Important:
  As part of the stages in building the pipeline, we include update image in which we will run a shell script that will update our image tags in a target directory.

  #!/bin/bash

set -x

# Set the repository URL
REPO_URL="https://<ACCESS-TOKEN>@dev.azure.com/<AZURE-DEVOPS-ORG-NAME>/voting-app/_git/voting-app"

# Clone the git repository into the /tmp directory
git clone "$REPO_URL" /tmp/temp_repo

# Navigate into the cloned repository directory
cd /tmp/temp_repo

# Make changes to the Kubernetes manifest file(s)
# For example, let's say you want to change the image tag in a deployment.yaml file
sed -i "s|image:.*|image: <ACR-REGISTRY-NAME>/$2:$3|g" k8s-specifications/$1-deployment.yaml

# Add the modified files
git add .

# Commit the changes
git commit -m "Update Kubernetes manifest"

# Push the changes back to the repository
git push

# Cleanup: remove the temporary directory
rm -rf /tmp/temp_repo

imagepullSecret:
  kubectl create secret docker-registry azurereg \
    --namespace default \
    --docker-server=<> \
    --docker-username=<> \
    --docker-password=<>
Go to the storage account and add role assignment..

Important:
Open port 30000-32676 for the nodePort in the nsg for your AKS.
type vmss --> virtual machine scale set and set the nsg rule by clickng on instance and network and inbound rule...


Important:
 Let's break down each part of the script to understand its purpose and how it operates:

1. **Shebang and Set Debug Mode**:
   ```bash
   #!/bin/bash

   set -x
   ```
   - The shebang `#!/bin/bash` at the beginning specifies that the script should be executed using Bash.
   - `set -x` turns on debugging mode, which causes Bash to display each command it executes. This is useful for understanding the flow of the script and debugging any issues.

2. **Set Repository URL**:
   ```bash
   REPO_URL="https://<ACCESS-TOKEN>@dev.azure.com/<AZURE-DEVOPS-ORG-NAME>/voting-app/_git/voting-app"
   ```
   - This line sets the URL of the Azure DevOps Git repository. You need to replace `<ACCESS-TOKEN>` and `<AZURE-DEVOPS-ORG-NAME>` with your actual access token and organization name, respectively.

3. **Clone the Git Repository**:
   ```bash
   git clone "$REPO_URL" /tmp/temp_repo
   ```
   - This command clones the Git repository specified by `REPO_URL` into the `/tmp/temp_repo` directory.

4. **Navigate into the Cloned Repository Directory**:
   ```bash
   cd /tmp/temp_repo
   ```
   - This command changes the current working directory to the directory where the repository was cloned (`/tmp/temp_repo`).

5. **Make Changes to Kubernetes Manifest File(s)**:
   ```bash
   sed -i "s|image:.*|image: <ACR-REGISTRY-NAME>/$2:$3|g" k8s-specifications/$1-deployment.yaml
   ```
   - This command uses `sed` (stream editor) to replace occurrences of the image tag in the Kubernetes deployment manifest file (`$1-deployment.yaml`).
   - The placeholder `$1` represents the deployment name, `$2` represents the ACR registry name, and `$3` represents the image tag. They are replaced accordingly.

6. **Add Modified Files**:
   ```bash
   git add .
   ```
   - This command stages all modified files in the repository for the next commit.

7. **Commit the Changes**:
   ```bash
   git commit -m "Update Kubernetes manifest"
   ```
   - This command commits the staged changes with the commit message "Update Kubernetes manifest".

8. **Push Changes to Repository**:
   ```bash
   git push
   ```
   - This command pushes the committed changes to the remote repository.

9. **Cleanup: Remove Temporary Directory**:
   ```bash
   rm -rf /tmp/temp_repo
   ```
   - This command removes the temporary directory `/tmp/temp_repo`, which was used for cloning the repository and making changes.

That's the breakdown of the script. It automates the process of updating Kubernetes manifest files in an Azure DevOps Git repository, making it convenient for deployment workflows.

NB: I made changes to the app.py to indicate summer vs winter.. as a developer..


 AKS vs Self Managed Kubernetes Clusters:
--------------------------------------------
We have 3 ways in which we can create our k8s cluster.
1. Azure managed Kubernetes Cluster.
2. Self managed kubernetes cluster using vm created in Azure cloud.
3. Self managed kubernetes cluster created in on-premise data center.
What option is best?

NOTE THESE FACTS:
----------------------
Design and Install a Kubernetes Cluster::;
Before you design a cluster, you must ask relevant questions..

1. Purpose
a. Education
Use--> Minikube
Single node cluster with kubeadm/GCP/AWS

b Development and testing
Multi-node cluster with a single master and multi workers.
setup using kubeadm/GCP/AWS/AKS

c. Hosting production applications
High avai. multi node cluster with multi master node.
setup using kubeadm/GCP/AWS/AKS/kops or other supported platform
upto 5000 nodes
upto 150,000 pods per cluster
upto 300,000 total containers
upto 100 pods per node.

2. Cloud or Onprem
3. Workloads
a. How many application are going to be hosted on the cluster
b. What kind of application will be hosted on the cluster.
i. Web application
ii. Big Data/Analytics

4. Application Resource Requirments
  i. CPU intensive
  ii. Memory intensive

  5. Traffic
  i. Heavy traffic
  ii. Burst traffic





  Choosing Kubernetes Infrastructure:
  turnkey solutions:
  1. vagrant
  2. vmware cloud pks
  3 openshift
  4. cloud foundry cr

  hosted solutions:
  1 GKE
  2 EKS
  3. AKS
  4. OPENSHIFT ONLINE

Deploy an E-Commerce Project on Azure Kubernetes Service:
This is an e-commerce application, we have to first understand the architecture of the app.
Stan's Robot Shop is a sample microservice application you can use as a sandbox to test and learn containerised application orchestration and monitoring techniques.
It is not intended to be a comprehensive reference example of how to write a microservices application,
although you will better understand some of those concepts by playing with Stan's Robot Shop.
To be clear, the error handling is patchy and there is not any security built into the application.

The list of microservices that makes up the stans Robot shop.
1. User microservice --> Takes care of the user registration.
Once the users registers, there is a validation to check the email and the user details is recorded in the database.
The user then logs in with his registered credential, which is validated in the database.
so there is a db that records the user input from the user microservice.

2. catalouge microservice: Within this catalouge theres an image etc

3. cart microservice: say you like any of the robots in the catalouge, you can select it and add it to the cart.
even though i login or logout the information has to be preserved in the cart.
when i checkout, it will no longer be in the cart but the purchased item..
4. payment microservice: This microservice handles the payment, once you go to the cart and finish it, youll be redirected to the payment microservice..

5. shiiping microservice: This calculates the distance the item will be shipped based on the address and the amount

6. dispatch microservice: Once payment is confirmed a item will be shipped..

7. web microservice: This can be nginx or apache for deploying the app.
8. ratings microservice: gives you ratings relating to each robot or item..

Important:
-------------
The user microservice, the catalouge uses mongodb db
The cart microservice uses redis for in Memory
the entire project uses mysql to store images.

The use of the various db solution is to give you a better experience on how these dbs work.
why will one choose redis db against mongodb or mysql: This can be due to application requirements, say we have a ratings application.
you understand that ratings changes, you can check a product with 100 ratings and a few min later its 200 theyre dynamic..
using redis which is an in memory db or caching mechanism is ideal b/c redis is fast in retreiving informations.
if you put those ratings in a mysql or mongodb db, there will be latency in retreiving it.
but with redis which is an in memory db it will be faster to retreive and compute data.

What will happen if the in memory db pod like redis goes down? In k8s we have pv and pvc, that will hold the data and once another redis pods is redeployed it will access the
the data in the pv and be in sync.,.

when should i go for azure files or azure disk?
1. azure disk: if your storage is accessed by a single pod use azure disk which is ebs in aws. with accessmode as readWriteOnce
2. azure files: if your storage is accessed by multiple pods in diff nodes, use azure files.. nfs in aws and acessmode should be readWriteMany

Sometimes you might want to use external storage solutions like
1. Netapp etc, then you will have to create csi driver which will enable you connect and use those external storage solutions..

Important:
  In AKS you can enable an ingress controller,
  Click on the cluster ---> networking and select enable ingress controller..
  It is very easy on aks but in aws you will have to deploy the ingress controller and configure it..
  Say in eks you have to deploy csi drivers but these things come inbuilt in azure kubernetes cluster.
  for aks the ingress controller name or ingressClassName is azure-application-gateway

  Configuration of the application..
  1. Create resource group
  2. create vm for the runner and install dependencies
  3. create k8s cluster

  ISSUES WITH DEPLOYMENT:
  https://github.com/helm/helm/issues/7264
  The answer i got:
  Check the chart for any stray files that may have crept in there. Usually this can occur because there may be some files that are being included as part of the chart.
  my answer: That was exactly the problem.. I had so tar files in the directly containing the chart.yaml.

  Azure DevOps Scenario
  -------------------------

Creating Effective Monitoring System on Azure, Monitor, Prometheus, Network Watcher
What exactly does a Devops or sre engineer monitor?
do they just monitor cpu or memory utilization by your application on a node? no.
for an end user to access an application there are diff chains or component that it has to access to get a response or reach the application.
these various aspect of our app has to be monitored for optimal performance. This way you will build an effective monitoring tools..
1. you have the vnet and its components such as firewall,subnet,nsg etc.
2. The cluster and its components, the apiserver,etcd,scheduler,kube-controller-manager, the worker node component and the applications running in them.
3. storage solutions such as blob,azure disk,azure files etc.
4. vm, serverless fxs and lots of more resources..

it is my responsibility as a devops engineer to setup these network or infrastructure and monitor them.
build an effective monitoring framework to monitor the built infrastructure and it components or services.

important:
monitoring is divided into 6 layers..
1. network traffic or components monitoring: if the network component are not responsive or working as intended, users will not be able to access the application in the cluster.
this results in bad user experience.
2. nodes and node pools: in aks the nodepools are created using vm scale sets. monitoring of the node is essential.
check if the vmss is functioning accordingly. are the node reaching 100% cpu/memory useage.. if this is the case your node wont work as intended bc it has reahed it max capacity.

for 1 and 2 above we collect these metrics and perform analysis and send an alert if the components are not performing well.
3. controlePlane: Monitoring of controlplane component is very important, you might say aks manages that and therefor it must be ok.
When there is an issue with the apiserver, etcd and scheduler etc.. There will be a bad user experience. This is why you should monitor those components..

4. The dataplane component: You can monitor the pods/containers to ensure that it is working as expected.

5. APM Monitoring: application performance monitoring, if this is not monitored it will result in bad user experience.
say your application is not responding to 100s of request even though the application gateway is working properly, it will result to bad user experience.

6. vm, storage accounts and serverless fxns monitoring:

Important:
what are the tools available for monitoring?
Can i use a single tool for all of them? the answer is yes but that is not advisable...
The tool you might be using to monitor application performance might be working but it might not be the best tool...
It is best to pick the tool that is best for each level of monitoring..

THINGS TO MONITOR AND RECOMMENDED TOOLS FOR THE MONITORING
1. network traffic or components monitoring: use Network watcher in azure, This can monitor NSG, vnet, ip flow vpn, firewall, WAF
2.  nodes and node pools: Say cpu, memory use Prometheus.. azure has managed prometheus by azure.

3 controlePlane: use azure monitor,
4. The dataplane component: container/pods use prometheus
5. APM Monitoring: use application insight which is part of azure-monitor..

6. vm, storage accounts and serverless fxns monitoring: use azure monitor, in azure monitor you will see storage act and other services simply link it for monitoring.
also integrate your vm etc..


 Azure Key Vault Integration with AKS.
 Kubernetes Secret Store CSI Driver with Azure Vault.
 How a secret stored in azure key vault can be made available to a pod in the k8s cluster.

 1. key vault is a secret management service in azure that is used to store sensitive informations like secrets, api keys, tokens and certificates.
 in most applications, ex k8s, cicd tools like github actions they have environment variable and secret stores for storing sensitive data.

 why should we use key vault?
 1. it is a centralized solution where you can store secrets and any sensitive data.
 2. secrets or data stored in key vault can be rotated. Adhering to security best practices every 90 days or less.
 3. when your secret, api token, database credentials, certificates are spread accross diff tools. How can you manage them and rotate them in a timely manner?
 This is why we need a centralized secret management solution.

 Key-vault comes with RBACK and it can be integrated with managed identities and service principle.
 Once you understand the keyvalut demo, you can integrate the azure cicd secrets using keyvault.
 all your secret in terraform, python, ansible can be stored in azure key vault.

 IMPLEMENTATION AND MORE CONCEPT ON CSI DRIVER.

 # Connect your Azure ID to the Azure Key Vault Secrets Store CSI Driver
### Configure workload identity
### Exporting some environment variables
```
export SUBSCRIPTION_ID=fe4a1fdb-6a1c-4a6d-a6b0-dbb12f6a00f8
export RESOURCE_GROUP=keyvault-demo
export UAMI=azurekeyvaultsecretsprovider-keyvault-demo-cluster
export KEYVAULT_NAME=aks-demo-abhi
export CLUSTER_NAME=keyvault-demo-cluster

az account set --subscription $SUBSCRIPTION_ID
```
 1. Say we have a use case where we have to store our cicd secrets in a key vault instance and refernce it in our pipeline.
 2. A use case where we have a pods in AKS that needs access to secrets stored in Azure key vault.

 we will deal with number 2, how can this be achieved? pod is in the kubernetes cluster and key vault is outside the kubernetes cluster.
 This involves many concepts:
 1. we will use secret store csi driver. normally csi drivers are used to mount external storage solutions into our kubernetes pods.
 say your pod wants to use storage from external providers like azure files, netapp, aws nfs, azure disk etc. you want to get the storage from these vendors and then mount it
 as a volume in your kubernetes pods, we use the concept of pv but to create the volume outside your kubernetes cluster you need to install a csi driver.
 how is this done?
 kubernetes has a csi interface or csi API. The storage solutions has to create the csi driver. so as a devops engineer its your responsibility to integrate the csi driver
 with kubernetes api to request for a particular volume.
 how does this work? when the developer requests for a pv using pvc via a sc this requests goes through the k8s csi api and creates a volume in the target providers platform
 and this is then mounted as a volume to the pods.

 3 What about secret store csi driver?
 The secret store csi driver is a special type of csi driver used in k8s to retreive secret from external secret storage solutions suchs as hcl vault,
azure key vault aws secret manager. The retreived secret is mounted to a pod as volume where it then reads the secret.
Implementation:
  1. Install the secret store csi driver in the k8s cluster.
  2. Install the secret provider plugin in the k8s cluster eg. hcl vault plugin, azure vault plugin or aws secret manager plugin for the csi driver to know the secret provider
  that it will target to get secret from..
In creating aks, you have an option to enable secret store csi driver as an add on, azure key vault provider as addon <run k get pods -n kube-system to see the pods>.
 In this case the kubernetes pod will be able to talk to azure key vault provider to get secret.

 That is not all, remember for a service to talk to another service in azure it needs a managed identities or service principle.
 3. we create a managed identities with permission to read and get secret from keyvault, using this managed identity, it will allow the specific pod talk to the target key vault instance in azure.
 There might be 100s of pods in the k8s cluster but we want a particular pod to access this secret, what do we do?
 we will map or integrate the managed identity created to the service account of the pod. This way the pod assigned the sa will perform the action of accessing the secret.

### Create a managed identity and export some parameters.

```
az identity create --name $UAMI --resource-group $RESOURCE_GROUP

export USER_ASSIGNED_CLIENT_ID="$(az identity show -g $RESOURCE_GROUP --name $UAMI --query 'clientId' -o tsv)"
export IDENTITY_TENANT=$(az aks show --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP --query identity.tenantId -o tsv)

 4. We use the oidc concept to integrate sa with managed identities..
 To connect the serviceaccount of kubernetes pod to the azure managed identities we need an OIDC Provider.
 it is important to know at this point that the managed identities has the permission to get secret from azure key vault and when the sa of pod is linked to it the sa
 inherits the permission and when the sa is assigned to a pod the pod inherits the permission and is able to read and get secrets from azure key vault.
 Note: When we created the cluster we enabled oidc issuer, we will need to export some of details of the oidc to be used in the next step.

 ### Get the AKS cluster OIDC Issuer URL
```
export AKS_OIDC_ISSUER="$(az aks show --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME --query "oidcIssuerProfile.issuerUrl" -o tsv)"
echo $AKS_OIDC_ISSUER
```
### Create the service account for the pod
```
export SERVICE_ACCOUNT_NAME="workload-identity-sa"
export SERVICE_ACCOUNT_NAMESPACE="default"
```

```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    azure.workload.identity/client-id: ${USER_ASSIGNED_CLIENT_ID}
  name: ${SERVICE_ACCOUNT_NAME}
  namespace: ${SERVICE_ACCOUNT_NAMESPACE}
EOF
```

### Setup Federation

```
export FEDERATED_IDENTITY_NAME="aksfederatedidentity"

az identity federated-credential create --name $FEDERATED_IDENTITY_NAME --identity-name $UAMI --resource-group $RESOURCE_GROUP --issuer ${AKS_OIDC_ISSUER} --subject system:serviceaccount:${SERVICE_ACCOUNT_NAMESPACE}:${SERVICE_ACCOUNT_NAME}
```

FINALLY:
We will create a secret providerclass and in the secret provider class we will mention what secret in key vault we want to access.
Then mount the secret providerclass as a volume to the pod that has the sa that should access azure key vault to read secret.
### Create the Secret Provider Class

```
cat <<EOF | kubectl apply -f -
# This is a SecretProviderClass example using workload identity to access your key vault
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: azure-kvname-wi # needs to be unique per namespace
spec:
  provider: azure
  parameters:
    usePodIdentity: "false"
    clientID: "${USER_ASSIGNED_CLIENT_ID}" # Setting this to use workload identity
    keyvaultName: ${KEYVAULT_NAME}       # Set to the name of your key vault
    cloudName: ""                         # [OPTIONAL for Azure] if not provided, the Azure environment defaults to AzurePublicCloud
    objects:  |
      array:
        - |
          objectName: secret1             # Set to the name of your secret
          objectType: secret              # object types: secret, key, or cert
          objectVersion: ""               # [OPTIONAL] object versions, default to latest if empty
        - |
          objectName: key1                # Set to the name of your key
          objectType: key
          objectVersion: ""
    tenantId: "${IDENTITY_TENANT}"        # The tenant ID of the key vault
EOF
```

 important: when creating azure key vault enable rbac so you can configure accesses on the key vault.
 make sure that the managed identities created has the right permission to retreive secret from key vault.
 get the key vault scope and use it to map to the managed identity.. This can also be done from the UI.
run,
### Create a role assignment that grants the workload ID access the key vault
```
export KEYVAULT_SCOPE=$(az keyvault show --name $KEYVAULT_NAME --query id -o tsv)

az role assignment create --role "Key Vault Administrator" --assignee $USER_ASSIGNED_CLIENT_ID --scope $KEYVAULT_SCOPE


VERIFICATION OF WORK:
---------------------------------------
# Verify Keyvault AKS Integration

### Create a sample pod to mount the secrets

```
cat <<EOF | kubectl apply -f -
# This is a sample pod definition for using SecretProviderClass and workload identity to access your key vault
kind: Pod
apiVersion: v1
metadata:
  name: busybox-secrets-store-inline-wi
  labels:
    azure.workload.identity/use: "true"
spec:
  serviceAccountName: "workload-identity-sa"
  containers:
    - name: busybox
      image: registry.k8s.io/e2e-test-images/busybox:1.29-4
      command:
        - "/bin/sleep"
        - "10000"
      volumeMounts:
      - name: secrets-store01-inline
        mountPath: "/mnt/secrets-store"
        readOnly: true
  volumes:
    - name: secrets-store01-inline
      csi:
        driver: secrets-store.csi.k8s.io
        readOnly: true
        volumeAttributes:
          secretProviderClass: "azure-kvname-wi"
EOF
```

### List the contents of the volume

```
kubectl exec busybox-secrets-store-inline-wi -- ls /mnt/secrets-store/
```

### Verify the contents in the file

```
kubectl exec busybox-secrets-store-inline -- cat /mnt/secrets-store/foo-secret
```


Introduction to Serverless using Azure Functions:
Real Time Usecases:
How a devops engineer implements cloud cost optimization for an organization..
Scaling and performance optimization
How a devops engineer manages cloud configuration against the compliance and security guidelines of an organization.
The term serverless does not mean no server but the dynamic provision and deletion of server whe needed.

use cases
--------------
say a developer is in need of blob storage. he creates the needed storage that allowed public access to the blob container and in the org theres a policy that requires that
all blob containers created should have public access disabled..
say also the hot and cold lifecycle mgt wasnt implemented per requirement in the creation of the storage.
1. this poses security risk of data in the blob due to the public access being enabled.
2. cost overhead is incured due to the non compliant with the lifecycle mgt policy.

We can you serverless fxn in azure to implement this policys accross our account once an event occurs like creation of blob etc to check if best practices where followed.
It then perform action of deny, warning of policy or compliant violation or mutate the creation.
say blob containers are created once or twice in a month, we shouldnt have a stale vm running that has our python code and enforces this policy.
having a stale vm is not cost effective, we can have azure create vm that has our appcode or pythoncode for us each time a request is made to provision blob strorage
this then runs our code for compliance..

important:
azure will charge you based on the number of time the serverless or azure fxn is invoked..
why run a vm 247 if you dont need it? why run any infrastructure 247 when it is not needed.

say we have a java application which is hosted in a vm, when users makes a request to the java app, the java app talks
 to the db and retreives information and sends it back to the user. we can put this java app in a serverless fxn so that each time a request comes on the java app a
 serverless fxn will be invoked or a vm will be provisioned by azure to handle that request in micro sec.
 You will not be charged when your api or serverless fxn is idel.

 another use case:
 lets say for example we have a use case that files uploaded to the blob storage should not exceed 1gb.
 Users uploads files to the blob container. When file is equal or greater than 1 gb is uploaded a serverless fxn in azure is invoked which sends a warning to the user that
 his file size shouldnt be 1gb of size and the file deleted or compressed.

 use case of queue storage:
 Use Case: Handling User Requests Sequentially with Azure Queue Storage;
 say we put items in the queue say user requests, when there are multiple user requests these requests are stored in azure queue storage and processed on accordingly
 based on first come first server bases using azure serverless fxn.
 user request could be to access application and reteive data from  db or access github act and give me number of pull requests that are active.
Azure Queue Storage can be effectively used in scenarios where you need to handle asynchronous processing of user requests, especially when you want
to process them in a sequential manner (e.g., first-come, first-served). Here's how you can utilize Azure Queue Storage for the use case you provided:

### Use Case: Handling User Requests Sequentially with Azure Queue Storage

1. **Request Submission:**
   - Users submit requests to access your application or retrieve data from a database.
   - For example, a user requests data from the application or asks for the number of active pull requests from a GitHub repository.

2. **Enqueue Requests:**
   - Each user request is enqueued into an Azure Queue Storage.
   - The queue ensures that requests are stored in the order they are received, facilitating first-in, first-out (FIFO) processing.

3. **Azure Function for Processing:**
   - An Azure Function is triggered by messages in the queue.
   - This serverless function processes each request as it becomes available in the queue.
   - For instance, the function may retrieve data from the database or interact with the GitHub API to fetch the number of active pull requests.

4. **Sequential Processing:**
   - The Azure Function processes each request sequentially, following the order in which they were enqueued.
   - This ensures that requests are handled on a first-come, first-served basis, maintaining fairness and consistency.

5. **Response Handling:**
   - After processing, the Azure Function generates a response to the user request.
   - For example, it may return the requested data or information about the active pull requests.

6. **Dequeue Requests:**
   - Once processed, requests are dequeued from the Azure Queue Storage.
   - This prevents duplicate processing and ensures that each request is handled exactly once.

### Benefits of Using Azure Queue Storage:

- **Scalability:** Azure Queue Storage automatically scales to accommodate varying request loads, ensuring reliable performance.
- **Reliability:** Queue Storage provides high availability and durability for your queued messages, minimizing the risk of data loss.
- **Decoupling:** By decoupling request submission from processing, you can handle bursts of requests without overwhelming your application.
- **Cost-Effective:** Queue Storage is cost-effective for handling asynchronous workloads, as you only pay for the storage and data transfer incurred.

### Considerations:

- **Message Visibility Timeout:** Set an appropriate message visibility timeout to ensure that messages remain invisible to other
consumers while being processed by the Azure Function.
- **Dead-Letter Queue:** Configure a dead-letter queue to capture messages that cannot be processed after a certain number of attempts,
allowing for manual investigation and troubleshooting.

By leveraging Azure Queue Storage and Azure Functions, you can efficiently handle user requests in a sequential manner, ensuring fairness
and responsiveness in your application.

Important:
  in serverless fxn, you dont own the server when or if you dont use it.
  in vm once created and in running mode you will be charged.

  sample code:
  Below is an example Azure Function written in Python that monitors a blob container for file uploads.
  If a file with a size equal to or greater than 1GB is uploaded, the function sends a warning to the user and optionally deletes or compresses the file.

```python
import os
import logging
import azure.functions as func
from azure.storage.blob import BlobServiceClient

def main(blob: func.InputStream):
    # Get environment variables
    storage_connection_string = os.environ["STORAGE_CONNECTION_STRING"]
    warning_threshold = int(os.environ.get("WARNING_THRESHOLD", 1024))  # Size threshold in MB, default 1GB
    delete_on_warning = os.environ.get("DELETE_ON_WARNING", "false").lower() == "true"
    compress_on_warning = os.environ.get("COMPRESS_ON_WARNING", "false").lower() == "true"

    # Initialize logging
    logging.basicConfig(level=logging.INFO)

    # Get blob size
    blob_size_mb = blob.length / (1024 * 1024)  # Convert bytes to megabytes

    if blob_size_mb >= warning_threshold:
        logging.warning(f"Uploaded file size ({blob_size_mb} MB) exceeds warning threshold ({warning_threshold} MB)")

        # Send warning to user (Implement your logic here, e.g., email, push notification)

        # Optionally, delete or compress the file
        if delete_on_warning:
            logging.info("Deleting the file")
            blob_uri = os.environ["WEBSITE_HOSTNAME"] + blob.name
            delete_blob(storage_connection_string, blob_uri)
        elif compress_on_warning:
            logging.info("Compressing the file")
            # Implement file compression logic here

def delete_blob(storage_connection_string, blob_uri):
    blob_service_client = BlobServiceClient.from_connection_string(storage_connection_string)
    container_name, blob_name = blob_uri.split("/", 1)
    container_client = blob_service_client.get_container_client(container_name)
    blob_client = container_client.get_blob_client(blob_name)
    blob_client.delete_blob()

```

In this function:

- The function is triggered whenever a file is uploaded to the blob container. The trigger type is an input stream (`func.InputStream`).
- The function retrieves the size of the uploaded file and checks if it exceeds the warning threshold (1GB by default).
- If the file size exceeds the threshold, a warning is logged and the function optionally deletes or compresses the file.
- The Azure Storage connection string and other settings are retrieved from environment variables.
- If `DELETE_ON_WARNING` is set to `true`, the function deletes the file. If `COMPRESS_ON_WARNING` is set to `true`, the
function compresses the file (implement compression logic as needed).
- The `delete_blob` function is used to delete the file from the blob container.

To use this function, you'll need to create an Azure Function App, set up a blob trigger, and configure the necessary environment variables.
 Additionally, you may need to adjust the warning threshold and implement the logic for sending warnings to users.


 MORE USE Case
 -------------------
 Below is a Python function that demonstrates how you can implement the described use case using Azure Queue Storage and Azure Functions.
 This function assumes you have already set up an Azure Queue Storage and an Azure Function with a queue trigger.

```python
import os
import logging
from azure.storage.queue import QueueClient
from azure.functions import func

def process_user_requests(message: func.QueueMessage):
    # Retrieve the user request from the queue message
    user_request = message.get_body().decode('utf-8')
    logging.info(f"Processing user request: {user_request}")

    # Process the user request based on its content
    if "access application" in user_request:
        # Placeholder for application access logic
        logging.info("Accessing application and retrieving data from the database...")
        # Implement logic to access application and retrieve data from the database
        # For example:
        # - Access database
        # - Retrieve requested data

    elif "active pull requests" in user_request:
        # Placeholder for GitHub API access logic
        logging.info("Accessing GitHub API to fetch the number of active pull requests...")
        # Implement logic to access GitHub API and fetch the number of active pull requests
        # For example:
        # - Use GitHub API to fetch pull requests
        # - Count the number of active pull requests

    else:
        logging.warning("Unknown user request")

    # Delete the processed message from the queue
    message.delete()

def main(msg: func.QueueMessage):
    try:
        process_user_requests(msg)
    except Exception as e:
        logging.error(f"An error occurred while processing the user request: {e}")

if __name__ == "__main__":
    # Set up logging
    logging.basicConfig(level=logging.INFO)
    # Initialize the Azure Queue client
    connection_string = os.environ["AzureWebJobsStorage"]
    queue_name = os.environ["QueueName"]
    queue_client = QueueClient.from_connection_string(connection_string, queue_name)
    # Fetch messages from the queue and process them
    for message in queue_client.receive_messages():
        process_user_requests(message)
```

In this function:

- The function `process_user_requests` processes each user request retrieved from the Azure Queue message.
- Depending on the content of the user request, the function performs different actions,
such as accessing the application and retrieving data from the database or fetching the number of active pull requests from GitHub.
- After processing, the message is deleted from the queue to ensure it's not processed again.
- The `main` function is the entry point for Azure Functions and calls `process_user_requests` for each queue message received.
- Error handling is implemented to log any exceptions that occur during processing.
- The function is designed to be deployed as an Azure Function triggered by messages in
the Azure Queue Storage queue specified in the `AzureWebJobsStorage` environment variable.
- Logging is used to provide visibility into the processing of user requests and any errors encountered.

Before using this function, make sure to replace placeholders (such as application access logic and GitHub API access logic) with your actual implementation.
 Additionally, configure the Azure Function settings and environment variables according to your Azure environment and requirements.


Let's break down the Python code provided for the Azure Function that processes user requests stored in an Azure Queue Storage.

### Function Overview:
The function is designed to be triggered by messages in an Azure Queue Storage queue.
 It processes each user request stored as a message in the queue, performs the necessary actions based on the request content,
 and deletes the processed message from the queue.

### Key Components:

1. **Azure Function Entry Point (`main` function):**
   - This function serves as the entry point for the Azure Function and is triggered by messages in the
   Azure Queue Storage queue specified in the `AzureWebJobsStorage` environment variable.
   - It receives a `QueueMessage` object representing a message from the queue as input.
   - It calls the `process_user_requests` function to process each queue message.

2. **User Request Processing Function (`process_user_requests` function):**
   - This function processes each user request retrieved from the Azure Queue message.
   - It takes a `QueueMessage` object as input.
   - It retrieves the content of the message, which represents the user request.
   - It checks the content of the user request and performs different actions based on the request type:
     - If the request is to access the application and retrieve data from the database, it logs the action and potentially implements the application access logic.
     - If the request is to fetch the number of active pull requests from GitHub, it logs the action and potentially implements the GitHub API access logic.
     - If the request type is unknown, it logs a warning message.
   - After processing the request, it deletes the message from the queue to ensure it's not processed again.

3. **Error Handling:**
   - The code includes a `try-except` block to handle exceptions that may occur during request processing.
   - If an exception occurs, an error message is logged.

4. **Initialization and Logging:**
   - The code initializes logging to provide visibility into the processing of user requests and any errors encountered.
   - It sets up logging to output messages with different severity levels (INFO, WARNING, ERROR).

5. **Azure Queue Client Initialization:**
   - It initializes the Azure Queue client using the connection string retrieved from the `AzureWebJobsStorage` environment variable.
   - It fetches messages from the queue using the `receive_messages` method and processes them using the `process_user_requests` function.

### Deployment and Configuration:
Before deploying and running the function, you need to:
- Configure the Azure Function settings and environment variables, including the `AzureWebJobsStorage` connection string and the queue name.
- Replace the placeholder implementation with your actual logic for accessing the application and interacting with the GitHub API.
- Deploy the function to Azure using Azure Functions.

### Use Case:
This function is suitable for scenarios where you need to process user requests asynchronously,
such as accessing an application or interacting with external services like GitHub, and maintain a sequential processing order based on the order of request submission.

Note in a free tier azure subscription, the serverless fxn is slow.
azure fxn app: this is like resource group. before creating azure fxn you have to create azure fxn app to hold or group the azure functions.
similar to storage act which is used to group storages.. this ensures centralized rbac and other properties control.
there4 to create azure fxn you have to first create a azure fxn app then create azure fxn.

There are 3 ways to implement you azure fxns:
1. via the CLI OR OTHER editors
2. VIA THE UI or portal
3. VIA vscode desktop
Important: if you do not use the right architecture or os type or didnt use the right python version.
you might end up not seeing the 1,2,3 types or ways of implememting azure fxn when you access the azure fxn app.

examples of functions triggers:
1. blob trigger
2. cosmodb trigger
3. eventgride
4. http trigger
5. queue trigger
6. eventhub trigger
7. serverhub queue trigger
8 serverbus topic trigger
9. time trigger

important: when you see path option while creating az fxn, say targeting blob containers. it means you have to specify the target
blob container eg blob1/(name) where name is constant, its left that way and blob1 is the actual name of your blob container.
since we do not kwn the name of the file a user will upload, (name) will be overriden with the actual name of the users file.

NB: azure fxn has.
1. code + test: shows your azz fxn codes
2. monitors:  shows success count and failures --> logs and invocations
3. integration: shows you the fxn name, the trigger, output and input
4. fxn key: the password or keys of the az fxn